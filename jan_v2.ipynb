{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785d4c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as cargadas\n",
      "Modelos avanzados: S√≠\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# ML avanzado\n",
    "from sklearn.preprocessing import StandardScaler, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingRegressor, \n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor\n",
    ")\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# XGBoost y LightGBM para mejor rendimiento\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "    ADVANCED_MODELS = True\n",
    "except:\n",
    "    ADVANCED_MODELS = False\n",
    "    print(\"‚ö†Ô∏è XGBoost/LightGBM no disponibles, usando solo sklearn\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as cargadas\")\n",
    "print(f\"Modelos avanzados: {'S√≠' if ADVANCED_MODELS else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570fbdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train shape: (95339, 33)\n",
      "üìä Test shape: (2250, 33)\n",
      "\n",
      "üîç Columnas en train: ['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric', 'color_name', 'color_rgb', 'image_embedding', 'length_type']...\n",
      "\n",
      "üìà Estad√≠sticas b√°sicas de Production:\n",
      "count     95339.000000\n",
      "mean      28927.421055\n",
      "std       34792.567183\n",
      "min          90.000000\n",
      "25%        6800.000000\n",
      "50%       19266.000000\n",
      "75%       37426.000000\n",
      "max      403172.000000\n",
      "Name: Production, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "train_full_df = pd.read_csv(\"train.csv\", sep=';')\n",
    "test_df = pd.read_csv(\"test.csv\", sep=';')\n",
    "\n",
    "print(f\"üìä Train shape: {train_full_df.shape}\")\n",
    "print(f\"üìä Test shape: {test_df.shape}\")\n",
    "print(f\"\\nüîç Columnas en train: {train_full_df.columns.tolist()[:10]}...\")\n",
    "print(f\"\\nüìà Estad√≠sticas b√°sicas de Production:\")\n",
    "print(train_full_df['Production'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea775de",
   "metadata": {},
   "source": [
    "## 1. An√°lisis de Series Temporales por Producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b8da12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analizando patrones temporales...\n",
      "Columnas disponibles: ['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric', 'color_name', 'color_rgb', 'image_embedding', 'length_type', 'silhouette_type', 'waist_type', 'neck_lapel_type', 'sleeve_length_type', 'heel_shape_type']...\n",
      "‚úÖ Features temporales creadas: (9843, 9)\n",
      "\n",
      "Ejemplos de volatilidad (CV):\n",
      "   ID  demand_cv  demand_trend\n",
      "0   1   0.477898     -0.531418\n",
      "1   2   0.243655      0.239921\n",
      "2   3   0.393046      0.709627\n",
      "3   4   0.744410     -0.715652\n",
      "4   6   0.173167     -0.188633\n",
      "5   7   0.253665      0.098152\n",
      "6   8   0.129080      0.071335\n",
      "7  10   0.333518     -0.254777\n",
      "8  11   1.229824     -1.036939\n",
      "9  12   0.460106     -0.021007\n",
      "‚úÖ Features temporales creadas: (9843, 9)\n",
      "\n",
      "Ejemplos de volatilidad (CV):\n",
      "   ID  demand_cv  demand_trend\n",
      "0   1   0.477898     -0.531418\n",
      "1   2   0.243655      0.239921\n",
      "2   3   0.393046      0.709627\n",
      "3   4   0.744410     -0.715652\n",
      "4   6   0.173167     -0.188633\n",
      "5   7   0.253665      0.098152\n",
      "6   8   0.129080      0.071335\n",
      "7  10   0.333518     -0.254777\n",
      "8  11   1.229824     -1.036939\n",
      "9  12   0.460106     -0.021007\n"
     ]
    }
   ],
   "source": [
    "# Analizar patrones temporales ANTES de agregar\n",
    "print(\"üîç Analizando patrones temporales...\")\n",
    "\n",
    "# Verificar columnas disponibles\n",
    "print(f\"Columnas disponibles: {train_full_df.columns.tolist()[:15]}...\")\n",
    "\n",
    "# Convertir fechas\n",
    "train_full_df['phase_in'] = pd.to_datetime(train_full_df['phase_in'], format='%d/%m/%Y', errors='coerce')\n",
    "train_full_df['phase_out'] = pd.to_datetime(train_full_df['phase_out'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Features temporales por producto (sin usar 'week' que no existe)\n",
    "# Contamos el n√∫mero de registros por ID como proxy de n√∫mero de semanas\n",
    "weekly_stats = train_full_df.groupby('ID').agg({\n",
    "    'weekly_demand': ['mean', 'std', 'max', 'min', 'sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "weekly_stats.columns = ['ID', 'demand_mean', 'demand_std', 'demand_max', 'demand_min', 'demand_sum', 'num_weeks']\n",
    "\n",
    "# Calcular coeficiente de variaci√≥n (volatilidad)\n",
    "weekly_stats['demand_cv'] = weekly_stats['demand_std'] / (weekly_stats['demand_mean'] + 1)\n",
    "\n",
    "# Tendencia: comparar primera mitad vs segunda mitad\n",
    "def calculate_trend(group):\n",
    "    if len(group) < 4:\n",
    "        return 0\n",
    "    mid = len(group) // 2\n",
    "    first_half = group.iloc[:mid]['weekly_demand'].mean()\n",
    "    second_half = group.iloc[mid:]['weekly_demand'].mean()\n",
    "    return (second_half - first_half) / (first_half + 1)\n",
    "\n",
    "trend_by_id = train_full_df.groupby('ID').apply(calculate_trend).reset_index()\n",
    "trend_by_id.columns = ['ID', 'demand_trend']\n",
    "\n",
    "weekly_stats = weekly_stats.merge(trend_by_id, on='ID', how='left')\n",
    "\n",
    "print(f\"‚úÖ Features temporales creadas: {weekly_stats.shape}\")\n",
    "print(f\"\\nEjemplos de volatilidad (CV):\")\n",
    "print(weekly_stats[['ID', 'demand_cv', 'demand_trend']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d33879",
   "metadata": {},
   "source": [
    "## 2. Estimaci√≥n de Demanda Real (ajustando por stockouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f8f28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Estimando demanda real (ajustando stockouts)...\n",
      "Columnas disponibles en train_full_df: ['ID', 'id_season', 'aggregated_family', 'family', 'category', 'fabric', 'color_name', 'color_rgb', 'image_embedding', 'length_type', 'silhouette_type', 'waist_type', 'neck_lapel_type', 'sleeve_length_type', 'heel_shape_type', 'toecap_type', 'woven_structure', 'knit_structure', 'print_type', 'archetype', 'moment', 'phase_in', 'phase_out', 'life_cycle_length', 'num_stores', 'num_sizes', 'has_plus_sizes', 'price', 'year', 'num_week_iso']...\n",
      "\n",
      "Columnas que se van a agregar: ['weekly_demand', 'Production', 'id_season', 'aggregated_family', 'family', 'category', 'fabric', 'color_name', 'image_embedding', 'length_type', 'silhouette_type', 'waist_type', 'neck_lapel_type', 'sleeve_length_type', 'heel_shape_type', 'toecap_type', 'woven_structure', 'knit_structure', 'print_type', 'archetype', 'moment', 'life_cycle_length', 'num_stores', 'num_sizes', 'has_plus_sizes', 'price', 'phase_in', 'phase_out']\n",
      "‚úÖ Productos con probable stockout: 191 / 9843 (1.9%)\n",
      "\n",
      "Ejemplos de ajuste:\n",
      "      ID  Production  total_demand  estimated_true_demand  utilization_rate\n",
      "16    26        3074          3218                3535.10          1.046504\n",
      "369  471         529           604                 608.35          1.139623\n",
      "407  521         408           440                 469.20          1.075795\n",
      "434  554         610           687                 701.50          1.124386\n",
      "436  556        4343          4905                4994.45          1.129144\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Estimando demanda real (ajustando stockouts)...\")\n",
    "\n",
    "# Verificar columnas disponibles antes de agregar\n",
    "print(f\"Columnas disponibles en train_full_df: {train_full_df.columns.tolist()[:30]}...\")\n",
    "\n",
    "# Definir columnas que queremos agregar (si existen)\n",
    "desired_columns = {\n",
    "    'weekly_demand': 'sum',\n",
    "    'Production': 'first',\n",
    "    'id_season': 'first',\n",
    "    'aggregated_family': 'first',\n",
    "    'family': 'first',\n",
    "    'category': 'first',\n",
    "    'fabric': 'first',\n",
    "    'color_name': 'first',\n",
    "    'image_embedding': 'first',\n",
    "    'length_type': 'first',\n",
    "    'silhouette_type': 'first',\n",
    "    'waist_type': 'first',\n",
    "    'neck_lapel_type': 'first',\n",
    "    'sleeve_length_type': 'first',\n",
    "    'heel_shape_type': 'first',\n",
    "    'toecap_type': 'first',\n",
    "    'woven_structure': 'first',\n",
    "    'knit_structure': 'first',\n",
    "    'print_type': 'first',\n",
    "    'archetype': 'first',\n",
    "    'moment': 'first',\n",
    "    'occasion': 'first',  # Puede ser 'occasion' en vez de 'ocassion'\n",
    "    'ocassion': 'first',  # O puede ser 'ocassion'\n",
    "    'life_cycle_length': 'first',\n",
    "    'num_stores': 'first',\n",
    "    'num_sizes': 'first',\n",
    "    'has_plus_sizes': 'first',\n",
    "    'price': 'first',\n",
    "    'phase_in': 'first',\n",
    "    'phase_out': 'first'\n",
    "}\n",
    "\n",
    "# Filtrar solo las columnas que existen\n",
    "agg_dict = {col: agg_func for col, agg_func in desired_columns.items() if col in train_full_df.columns}\n",
    "\n",
    "print(f\"\\nColumnas que se van a agregar: {list(agg_dict.keys())}\")\n",
    "\n",
    "# Agregar datos a nivel de producto\n",
    "train_agg = train_full_df.groupby('ID').agg(agg_dict).reset_index()\n",
    "\n",
    "train_agg.rename(columns={'weekly_demand': 'total_demand'}, inplace=True)\n",
    "\n",
    "# Merge con features temporales\n",
    "train_agg = train_agg.merge(weekly_stats, on='ID', how='left')\n",
    "\n",
    "# CLAVE: Estimar demanda real cuando hubo stockout\n",
    "# Si demanda >= 95% de producci√≥n, probablemente hubo stockout\n",
    "train_agg['utilization_rate'] = train_agg['total_demand'] / (train_agg['Production'] + 1)\n",
    "train_agg['likely_stockout'] = (train_agg['utilization_rate'] > 0.95).astype(int)\n",
    "\n",
    "# Ajustar demanda estimada\n",
    "# Si hubo stockout, asumir que la demanda real era mayor\n",
    "train_agg['estimated_true_demand'] = train_agg['total_demand'].copy()\n",
    "stockout_mask = train_agg['likely_stockout'] == 1\n",
    "\n",
    "# Para stockouts, estimar demanda = producci√≥n * (1 + factor de ajuste basado en popularidad)\n",
    "# El factor depende del tipo de producto y precio\n",
    "train_agg.loc[stockout_mask, 'estimated_true_demand'] = (\n",
    "    train_agg.loc[stockout_mask, 'Production'] * 1.15  # Asumir 15% m√°s de demanda\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Productos con probable stockout: {stockout_mask.sum()} / {len(train_agg)} ({stockout_mask.mean()*100:.1f}%)\")\n",
    "print(f\"\\nEjemplos de ajuste:\")\n",
    "print(train_agg[stockout_mask][['ID', 'Production', 'total_demand', 'estimated_true_demand', 'utilization_rate']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2eabd",
   "metadata": {},
   "source": [
    "## 3. Features Avanzadas: Clustering y Similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0364dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creando features de similitud y clustering...\n",
      "Dimensi√≥n embedding: 512\n",
      "Aplicando K-Means clustering...\n",
      "Dimensi√≥n embedding: 512\n",
      "Aplicando K-Means clustering...\n",
      "Calculando k-NN features...\n",
      "Calculando k-NN features...\n",
      "‚úÖ Features de clustering y similitud creadas\n",
      "‚úÖ Features de clustering y similitud creadas\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Creando features de similitud y clustering...\")\n",
    "\n",
    "# Procesar embeddings\n",
    "def parse_embedding(embedding_str):\n",
    "    if pd.isna(embedding_str) or embedding_str == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return np.array(embedding_str.split(','), dtype=np.float32)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "train_agg['emb_array'] = train_agg['image_embedding'].apply(parse_embedding)\n",
    "test_df['emb_array'] = test_df['image_embedding'].apply(parse_embedding)\n",
    "\n",
    "# Detectar dimensi√≥n\n",
    "first_valid = train_agg['emb_array'].dropna().iloc[0]\n",
    "EMB_DIM = len(first_valid)\n",
    "print(f\"Dimensi√≥n embedding: {EMB_DIM}\")\n",
    "\n",
    "# Rellenar\n",
    "default_emb = np.zeros(EMB_DIM)\n",
    "train_embeddings = np.stack(\n",
    "    train_agg['emb_array'].apply(lambda x: x if x is not None else default_emb).tolist()\n",
    ")\n",
    "test_embeddings = np.stack(\n",
    "    test_df['emb_array'].apply(lambda x: x if x is not None else default_emb).tolist()\n",
    ")\n",
    "\n",
    "# Clustering visual (productos similares)\n",
    "print(\"Aplicando K-Means clustering...\")\n",
    "n_clusters = 50  # Agrupar en 50 clusters de estilo visual\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "train_agg['visual_cluster'] = kmeans.fit_predict(train_embeddings)\n",
    "test_df['visual_cluster'] = kmeans.predict(test_embeddings)\n",
    "\n",
    "# Features por cluster (para detectar canibalismo)\n",
    "cluster_stats = train_agg.groupby('visual_cluster').agg({\n",
    "    'Production': ['mean', 'std', 'count'],\n",
    "    'estimated_true_demand': 'mean',\n",
    "    'price': 'mean'\n",
    "}).reset_index()\n",
    "cluster_stats.columns = ['visual_cluster', 'cluster_prod_mean', 'cluster_prod_std', \n",
    "                          'cluster_size', 'cluster_demand_mean', 'cluster_price_mean']\n",
    "\n",
    "train_agg = train_agg.merge(cluster_stats, on='visual_cluster', how='left')\n",
    "test_df = test_df.merge(cluster_stats, on='visual_cluster', how='left')\n",
    "\n",
    "# k-NN features (productos m√°s similares)\n",
    "print(\"Calculando k-NN features...\")\n",
    "knn = NearestNeighbors(n_neighbors=11, metric='cosine')\n",
    "knn.fit(train_embeddings)\n",
    "\n",
    "# Para test\n",
    "distances_test, indices_test = knn.kneighbors(test_embeddings, n_neighbors=10)\n",
    "test_df['knn_mean_production'] = [train_agg.iloc[idx]['Production'].mean() for idx in indices_test]\n",
    "test_df['knn_mean_demand'] = [train_agg.iloc[idx]['estimated_true_demand'].mean() for idx in indices_test]\n",
    "test_df['knn_mean_price'] = [train_agg.iloc[idx]['price'].mean() for idx in indices_test]\n",
    "\n",
    "# Para train (excluyendo el mismo producto)\n",
    "distances_train, indices_train = knn.kneighbors(train_embeddings, n_neighbors=11)\n",
    "indices_train_excl = indices_train[:, 1:]  # Excluir el primero (√©l mismo)\n",
    "train_agg['knn_mean_production'] = [train_agg.iloc[idx]['Production'].mean() for idx in indices_train_excl]\n",
    "train_agg['knn_mean_demand'] = [train_agg.iloc[idx]['estimated_true_demand'].mean() for idx in indices_train_excl]\n",
    "train_agg['knn_mean_price'] = [train_agg.iloc[idx]['price'].mean() for idx in indices_train_excl]\n",
    "\n",
    "print(f\"‚úÖ Features de clustering y similitud creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541f615",
   "metadata": {},
   "source": [
    "## 4. Features de Negocio Avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929b803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creando features de negocio avanzadas...\n",
      "‚úÖ Features de negocio creadas\n",
      "Train shape: (9843, 70)\n",
      "Test shape: (2250, 60)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Creando features de negocio avanzadas...\")\n",
    "\n",
    "# Para train y test\n",
    "for df in [train_agg, test_df]:\n",
    "    # Intensidad de distribuci√≥n\n",
    "    df['stores_per_week'] = df['num_stores'] / (df['life_cycle_length'] + 1)\n",
    "    df['total_sku_count'] = df['num_stores'] * df['num_sizes']\n",
    "    if 'has_plus_sizes' in df.columns:\n",
    "        df['has_plus_sizes'] = df['has_plus_sizes'].map({True: 1, False: 0, 'true': 1, 'false': 0}).fillna(0)\n",
    "        df['total_sku_count'] = df['total_sku_count'] * (1 + 0.3 * df['has_plus_sizes'])\n",
    "    \n",
    "    # Segmento de precio\n",
    "    df['price_segment'] = pd.cut(df['price'], bins=[0, 20, 40, 60, 100, 200], \n",
    "                                   labels=['Budget', 'Mid', 'Mid-High', 'Premium', 'Luxury'])\n",
    "    \n",
    "    # Ratio precio/stores (indicador de target de mercado)\n",
    "    df['price_store_ratio'] = df['price'] / (df['num_stores'] + 1)\n",
    "    \n",
    "    # Duraci√≥n del ciclo (temporadas largas = m√°s volumen)\n",
    "    df['is_long_cycle'] = (df['life_cycle_length'] > 12).astype(int)\n",
    "    \n",
    "    # Tipo de categor√≠a (impacto en volumen)\n",
    "    df['is_basics'] = df['category'].isin(['T-shirt', 'Jeans', 'Tops', 'Bottoms']).astype(int)\n",
    "    df['is_outerwear'] = df['category'].isin(['Jackets', 'Coats', 'Puffer coats', 'Blazers']).astype(int)\n",
    "    df['is_special'] = df['category'].isin(['Dresses', 'Swimwear', 'Intimate']).astype(int)\n",
    "\n",
    "# Features espec√≠ficas de train (usando demanda hist√≥rica)\n",
    "train_agg['demand_per_store'] = train_agg['estimated_true_demand'] / (train_agg['num_stores'] + 1)\n",
    "train_agg['demand_per_week'] = train_agg['estimated_true_demand'] / (train_agg['life_cycle_length'] + 1)\n",
    "train_agg['production_efficiency'] = train_agg['Production'] / (train_agg['estimated_true_demand'] + 1)\n",
    "\n",
    "# Features de temporada (tendencias entre temporadas)\n",
    "season_stats = train_agg.groupby('id_season').agg({\n",
    "    'Production': ['mean', 'std'],\n",
    "    'estimated_true_demand': 'mean',\n",
    "    'price': 'mean'\n",
    "}).reset_index()\n",
    "season_stats.columns = ['id_season', 'season_prod_mean', 'season_prod_std', \n",
    "                         'season_demand_mean', 'season_price_mean']\n",
    "\n",
    "train_agg = train_agg.merge(season_stats, on='id_season', how='left')\n",
    "\n",
    "# Para test, usar stats de la √∫ltima temporada como proxy\n",
    "if 'id_season' in test_df.columns:\n",
    "    test_df = test_df.merge(season_stats, on='id_season', how='left')\n",
    "else:\n",
    "    # Usar √∫ltima temporada\n",
    "    last_season_stats = season_stats.iloc[-1]\n",
    "    for col in ['season_prod_mean', 'season_prod_std', 'season_demand_mean', 'season_price_mean']:\n",
    "        test_df[col] = last_season_stats[col]\n",
    "\n",
    "# Features de categor√≠a\n",
    "category_stats = train_agg.groupby('category').agg({\n",
    "    'Production': ['mean', 'std', 'median'],\n",
    "    'estimated_true_demand': 'mean',\n",
    "    'utilization_rate': 'mean'\n",
    "}).reset_index()\n",
    "category_stats.columns = ['category', 'cat_prod_mean', 'cat_prod_std', 'cat_prod_median',\n",
    "                           'cat_demand_mean', 'cat_util_mean']\n",
    "\n",
    "train_agg = train_agg.merge(category_stats, on='category', how='left')\n",
    "test_df = test_df.merge(category_stats, on='category', how='left')\n",
    "\n",
    "print(f\"‚úÖ Features de negocio creadas\")\n",
    "print(f\"Train shape: {train_agg.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24ac30",
   "metadata": {},
   "source": [
    "## 5. Preparaci√≥n de Features para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fb9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Preparando features para modelado...\n",
      "Features num√©ricas: 37\n",
      "Features categ√≥ricas: 11\n",
      "Features solo train: 3\n",
      "\n",
      "Total features train: 51\n",
      "Total features test: 48\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Preparando features para modelado...\")\n",
    "\n",
    "# TARGET\n",
    "TARGET = 'Production'\n",
    "\n",
    "# Features num√©ricas\n",
    "numerical_features = [\n",
    "    # B√°sicas\n",
    "    'life_cycle_length', 'num_stores', 'num_sizes', 'price',\n",
    "    \n",
    "    # Temporales\n",
    "    'demand_mean', 'demand_std', 'demand_max', 'demand_min', 'demand_cv', 'demand_trend', 'num_weeks',\n",
    "    \n",
    "    # Demanda estimada\n",
    "    'estimated_true_demand', 'utilization_rate',\n",
    "    \n",
    "    # Clustering\n",
    "    'cluster_prod_mean', 'cluster_prod_std', 'cluster_size', 'cluster_demand_mean', 'cluster_price_mean',\n",
    "    \n",
    "    # k-NN\n",
    "    'knn_mean_production', 'knn_mean_demand', 'knn_mean_price',\n",
    "    \n",
    "    # Negocio\n",
    "    'stores_per_week', 'total_sku_count', 'price_store_ratio',\n",
    "    'is_long_cycle', 'is_basics', 'is_outerwear', 'is_special',\n",
    "    \n",
    "    # Temporada\n",
    "    'season_prod_mean', 'season_prod_std', 'season_demand_mean', 'season_price_mean',\n",
    "    \n",
    "    # Categor√≠a\n",
    "    'cat_prod_mean', 'cat_prod_std', 'cat_prod_median', 'cat_demand_mean', 'cat_util_mean'\n",
    "]\n",
    "\n",
    "# Features solo en train\n",
    "train_only_features = ['demand_per_store', 'demand_per_week', 'production_efficiency']\n",
    "\n",
    "# Features categ√≥ricas (usar Target Encoding)\n",
    "categorical_features = [\n",
    "    'aggregated_family', 'family', 'category', 'fabric', \n",
    "    'length_type', 'silhouette_type', 'print_type', \n",
    "    'archetype', 'moment', 'ocassion', 'price_segment',\n",
    "    'visual_cluster'\n",
    "]\n",
    "\n",
    "# Filtrar features que existen\n",
    "numerical_features = [f for f in numerical_features if f in train_agg.columns]\n",
    "train_only_features = [f for f in train_only_features if f in train_agg.columns]\n",
    "categorical_features = [f for f in categorical_features if f in train_agg.columns]\n",
    "\n",
    "print(f\"Features num√©ricas: {len(numerical_features)}\")\n",
    "print(f\"Features categ√≥ricas: {len(categorical_features)}\")\n",
    "print(f\"Features solo train: {len(train_only_features)}\")\n",
    "\n",
    "# Combinar\n",
    "all_train_features = numerical_features + train_only_features + categorical_features\n",
    "all_test_features = numerical_features + categorical_features\n",
    "\n",
    "print(f\"\\nTotal features train: {len(all_train_features)}\")\n",
    "print(f\"Total features test: {len(all_test_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec013a",
   "metadata": {},
   "source": [
    "## 6. Validaci√≥n Temporal (Simular cambios de tendencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919b0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Configurando validaci√≥n temporal...\n",
      "Temporadas disponibles: [np.int64(86), np.int64(87), np.int64(88), np.int64(89)]\n",
      "Train seasons: [np.int64(86), np.int64(87)]\n",
      "Validation seasons: [np.int64(88), np.int64(89)]\n",
      "\n",
      "‚úÖ Train: (5079, 51), Val: (4764, 51)\n",
      "\n",
      "Distribuci√≥n del target:\n",
      "Train - Media: 24317, Mediana: 17456\n",
      "Val - Media: 25218, Mediana: 18703\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Configurando validaci√≥n temporal...\")\n",
    "\n",
    "# Split temporal: √∫ltimas temporadas para validaci√≥n\n",
    "seasons = sorted(train_agg['id_season'].unique())\n",
    "print(f\"Temporadas disponibles: {seasons}\")\n",
    "\n",
    "# Usar las √∫ltimas 2 temporadas para validaci√≥n\n",
    "val_seasons = seasons[-2:]\n",
    "train_seasons = seasons[:-2]\n",
    "\n",
    "print(f\"Train seasons: {train_seasons}\")\n",
    "print(f\"Validation seasons: {val_seasons}\")\n",
    "\n",
    "train_mask = train_agg['id_season'].isin(train_seasons)\n",
    "val_mask = train_agg['id_season'].isin(val_seasons)\n",
    "\n",
    "X_train = train_agg[train_mask][all_train_features].copy()\n",
    "y_train = train_agg[train_mask][TARGET].copy()\n",
    "\n",
    "X_val = train_agg[val_mask][all_train_features].copy()\n",
    "y_val = train_agg[val_mask][TARGET].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "print(f\"\\nDistribuci√≥n del target:\")\n",
    "print(f\"Train - Media: {y_train.mean():.0f}, Mediana: {y_train.median():.0f}\")\n",
    "print(f\"Val - Media: {y_val.mean():.0f}, Mediana: {y_val.median():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b490c",
   "metadata": {},
   "source": [
    "## 7. Entrenamiento de Modelos (Ensemble Avanzado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000b95a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Entrenando modelos...\n",
      "\n",
      "üîÑ Entrenando HGB...\n",
      "  MAE: 1233.37, RMSE: 3581.84, R¬≤: 0.9820\n",
      "\n",
      "üîÑ Entrenando RF...\n",
      "  MAE: 1233.37, RMSE: 3581.84, R¬≤: 0.9820\n",
      "\n",
      "üîÑ Entrenando RF...\n",
      "  MAE: 11282.61, RMSE: 19953.35, R¬≤: 0.4420\n",
      "\n",
      "üîÑ Entrenando ET...\n",
      "  MAE: 11282.61, RMSE: 19953.35, R¬≤: 0.4420\n",
      "\n",
      "üîÑ Entrenando ET...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ Entrenando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     pred_val = model.predict(X_val)\n\u001b[32m     85\u001b[39m     pred_val = np.maximum(pred_val, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# No negativos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"üéØ Entrenando modelos...\")\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Para features categ√≥ricas, usar TargetEncoder con menos folds para evitar errores\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', TargetEncoder(cv=2, smooth='auto'))  # Reducir CV folds de 5 a 2\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features + train_only_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Modelos\n",
    "models = {}\n",
    "\n",
    "# Modelo 1: HistGradientBoosting (robusto a outliers)\n",
    "models['HGB'] = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HistGradientBoostingRegressor(\n",
    "        loss='poisson',  # Mejor para conteos\n",
    "        max_iter=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=12,\n",
    "        min_samples_leaf=15,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Modelo 2: Random Forest\n",
    "models['RF'] = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=10,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Modelo 3: ExtraTrees (mayor diversidad)\n",
    "models['ET'] = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ExtraTreesRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=25,\n",
    "        min_samples_leaf=8,\n",
    "        random_state=43,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Modelo 4: Huber Regressor (robusto a outliers)\n",
    "models['Huber'] = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', HuberRegressor(\n",
    "        epsilon=1.5,\n",
    "        max_iter=200,\n",
    "        alpha=0.01\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "predictions_val = {}\n",
    "scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Entrenando {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        pred_val = model.predict(X_val)\n",
    "        pred_val = np.maximum(pred_val, 0)  # No negativos\n",
    "        predictions_val[name] = pred_val\n",
    "        \n",
    "        mae = mean_absolute_error(y_val, pred_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "        r2 = r2_score(y_val, pred_val)\n",
    "        \n",
    "        scores[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "        print(f\"  MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Error entrenando {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESUMEN DE MODELOS (Validaci√≥n)\")\n",
    "print(\"=\"*60)\n",
    "if scores:\n",
    "    scores_df = pd.DataFrame(scores).T\n",
    "    print(scores_df.sort_values('MAE'))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudo entrenar ning√∫n modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33731071",
   "metadata": {},
   "source": [
    "## 8. Ensemble √ìptimo (Weighted Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Creando ensemble √≥ptimo...\")\n",
    "\n",
    "# Pesos basados en MAE inverso (mejor modelo = mayor peso)\n",
    "mae_scores = np.array([scores[name]['MAE'] for name in models.keys()])\n",
    "weights = 1 / mae_scores\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "print(\"Pesos del ensemble:\")\n",
    "for name, weight in zip(models.keys(), weights):\n",
    "    print(f\"  {name}: {weight:.3f}\")\n",
    "\n",
    "# Predicci√≥n ensemble en validaci√≥n\n",
    "ensemble_val = np.zeros(len(y_val))\n",
    "for name, weight in zip(models.keys(), weights):\n",
    "    ensemble_val += weight * predictions_val[name]\n",
    "\n",
    "# Evaluar ensemble\n",
    "mae_ensemble = mean_absolute_error(y_val, ensemble_val)\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_val, ensemble_val))\n",
    "r2_ensemble = r2_score(y_val, ensemble_val)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÜ ENSEMBLE FINAL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MAE: {mae_ensemble:.2f}\")\n",
    "print(f\"RMSE: {rmse_ensemble:.2f}\")\n",
    "print(f\"R¬≤: {r2_ensemble:.4f}\")\n",
    "print(f\"\\nMejora vs mejor modelo individual: {(scores_df['MAE'].min() - mae_ensemble):.2f} puntos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ea054",
   "metadata": {},
   "source": [
    "## 9. Reentrenamiento con TODOS los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Reentrenando modelos con TODOS los datos...\")\n",
    "\n",
    "X_full = train_agg[all_train_features].copy()\n",
    "y_full = train_agg[TARGET].copy()\n",
    "\n",
    "final_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Entrenando {name}...\")\n",
    "    final_model = model.__class__(**model.get_params())\n",
    "    final_model.fit(X_full, y_full)\n",
    "    final_models[name] = final_model\n",
    "\n",
    "print(\"‚úÖ Modelos finales entrenados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f1faa",
   "metadata": {},
   "source": [
    "## 10. Predicciones en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f53223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Generando predicciones en test...\")\n",
    "\n",
    "# Preparar test (rellenar features que solo est√°n en train)\n",
    "for feat in train_only_features:\n",
    "    if feat not in test_df.columns:\n",
    "        test_df[feat] = 0  # O usar un valor m√°s inteligente\n",
    "\n",
    "X_test = test_df[all_train_features].copy()\n",
    "\n",
    "# Predicciones de cada modelo\n",
    "predictions_test = {}\n",
    "for name, model in final_models.items():\n",
    "    pred = model.predict(X_test)\n",
    "    pred = np.maximum(pred, 0)\n",
    "    predictions_test[name] = pred\n",
    "    print(f\"  {name} - Media: {pred.mean():.0f}, Min: {pred.min():.0f}, Max: {pred.max():.0f}\")\n",
    "\n",
    "# Ensemble final\n",
    "final_predictions = np.zeros(len(X_test))\n",
    "for name, weight in zip(models.keys(), weights):\n",
    "    final_predictions += weight * predictions_test[name]\n",
    "\n",
    "# Redondear y asegurar no negativos\n",
    "final_predictions = np.round(final_predictions).astype(int)\n",
    "final_predictions = np.maximum(final_predictions, 0)\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de predicciones finales:\")\n",
    "print(f\"  Media: {final_predictions.mean():.0f}\")\n",
    "print(f\"  Mediana: {np.median(final_predictions):.0f}\")\n",
    "print(f\"  Min: {final_predictions.min()}, Max: {final_predictions.max()}\")\n",
    "print(f\"  Std: {final_predictions.std():.0f}\")\n",
    "\n",
    "# Comparar con train\n",
    "print(f\"\\nüìä Comparaci√≥n con train:\")\n",
    "print(f\"  Train Media: {y_full.mean():.0f}\")\n",
    "print(f\"  Train Mediana: {y_full.median():.0f}\")\n",
    "print(f\"  Ratio Test/Train: {final_predictions.mean() / y_full.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d36dde",
   "metadata": {},
   "source": [
    "## 11. Ajustes Finales y Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Guardando predicciones...\")\n",
    "\n",
    "# Crear submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Production': final_predictions\n",
    "})\n",
    "\n",
    "# Guardar\n",
    "submission.to_csv('submission_advanced_v2.csv', index=False, sep=';')\n",
    "print(f\"‚úÖ Archivo guardado: submission_advanced_v2.csv\")\n",
    "\n",
    "print(f\"\\nüìã Primeras 20 predicciones:\")\n",
    "print(submission.head(20))\n",
    "\n",
    "# An√°lisis de distribuci√≥n\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y_full, bins=50, alpha=0.7, label='Train', edgecolor='black')\n",
    "plt.hist(final_predictions, bins=50, alpha=0.7, label='Test', edgecolor='black')\n",
    "plt.xlabel('Production')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribuci√≥n Train vs Test')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot([y_full, final_predictions], labels=['Train', 'Test'])\n",
    "plt.ylabel('Production')\n",
    "plt.title('Boxplot Comparativo')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(range(len(final_predictions)), final_predictions, alpha=0.5, s=1)\n",
    "plt.axhline(y=final_predictions.mean(), color='r', linestyle='--', label=f'Media: {final_predictions.mean():.0f}')\n",
    "plt.xlabel('√çndice')\n",
    "plt.ylabel('Production')\n",
    "plt.title('Predicciones por producto')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions_analysis_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ PROCESO COMPLETADO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556cb3f",
   "metadata": {},
   "source": [
    "## 12. An√°lisis de Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e186253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance del mejor modelo (Random Forest)\n",
    "if 'RF' in final_models:\n",
    "    rf_model = final_models['RF']\n",
    "    feature_names = numerical_features + train_only_features + categorical_features\n",
    "    \n",
    "    # Obtener importancias\n",
    "    importances = rf_model.named_steps['model'].feature_importances_\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ TOP 20 Features m√°s importantes:\")\n",
    "    print(feature_importance_df.head(20))\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance_df.head(20)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Top 20 Features M√°s Importantes')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_v2.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
