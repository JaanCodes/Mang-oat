{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b3caaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importació de llibreries bàsiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast  # Per processar els embeddings\n",
    "from pathlib import Path\n",
    "\n",
    "# Preprocessament i Modelització\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43164d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dades carregades:\n",
      "Train:   (95339, 33)\n",
      "Test:    (2250, 33)\n",
      "Sample:  (2250, 2)\n"
     ]
    }
   ],
   "source": [
    "# Càrrega de dades\n",
    "data_dir = Path('.')\n",
    "train_path = data_dir / 'train.csv'\n",
    "test_path = data_dir / 'test.csv'\n",
    "sample_path = data_dir / 'sample_submission.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path, sep=';')\n",
    "    test_df = pd.read_csv(test_path, sep=';')\n",
    "    sample_sub = pd.read_csv(sample_path, sep=',')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Assegura't que els fitxers 'train.csv', 'test.csv' i 'sample_submission.csv' estan al mateix directori.\")\n",
    "\n",
    "print(\"Dades carregades:\")\n",
    "print(f\"Train:   {train_df.shape}\")\n",
    "print(f\"Test:    {test_df.shape}\")\n",
    "print(f\"Sample:  {sample_sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "410ca891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vistazo a train_df (head) ---\n",
      "   ID  id_season      aggregated_family   family  \\\n",
      "0   1         86  Dresses and jumpsuits  Dresses   \n",
      "1   1         86  Dresses and jumpsuits  Dresses   \n",
      "2   1         86  Dresses and jumpsuits  Dresses   \n",
      "3   1         86  Dresses and jumpsuits  Dresses   \n",
      "4   1         86  Dresses and jumpsuits  Dresses   \n",
      "\n",
      "                              category fabric color_name  color_rgb  \\\n",
      "0  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "1  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "2  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "3  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "4  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "\n",
      "                                     image_embedding length_type  \\\n",
      "0  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "1  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "2  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "3  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "4  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "\n",
      "  silhouette_type waist_type neck_lapel_type sleeve_length_type  \\\n",
      "0           Evase        NaN          V-Neck         Fine Strap   \n",
      "1           Evase        NaN          V-Neck         Fine Strap   \n",
      "2           Evase        NaN          V-Neck         Fine Strap   \n",
      "3           Evase        NaN          V-Neck         Fine Strap   \n",
      "4           Evase        NaN          V-Neck         Fine Strap   \n",
      "\n",
      "   heel_shape_type  toecap_type woven_structure knit_structure     print_type  \\\n",
      "0              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "1              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "2              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "3              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "4              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "\n",
      "  archetype    moment    phase_in   phase_out  life_cycle_length  num_stores  \\\n",
      "0       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "1       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "2       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "3       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "4       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "\n",
      "   num_sizes  has_plus_sizes  price  year  num_week_iso  weekly_sales  \\\n",
      "0          5           False  35.99  2023             1            66   \n",
      "1          5           False  35.99  2023             2           112   \n",
      "2          5           False  35.99  2023             3           135   \n",
      "3          5           False  35.99  2023             4            99   \n",
      "4          5           False  35.99  2023             5            74   \n",
      "\n",
      "   weekly_demand  Production  \n",
      "0             69        4556  \n",
      "1            112        4556  \n",
      "2            135        4556  \n",
      "3             99        4556  \n",
      "4             74        4556  \n",
      "\n",
      "\n",
      "--- Información de train_df (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95339 entries, 0 to 95338\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  95339 non-null  int64  \n",
      " 1   id_season           95339 non-null  int64  \n",
      " 2   aggregated_family   95339 non-null  object \n",
      " 3   family              95339 non-null  object \n",
      " 4   category            95339 non-null  object \n",
      " 5   fabric              95339 non-null  object \n",
      " 6   color_name          95339 non-null  object \n",
      " 7   color_rgb           95339 non-null  object \n",
      " 8   image_embedding     95339 non-null  object \n",
      " 9   length_type         86830 non-null  object \n",
      " 10  silhouette_type     82972 non-null  object \n",
      " 11  waist_type          23252 non-null  object \n",
      " 12  neck_lapel_type     58874 non-null  object \n",
      " 13  sleeve_length_type  57336 non-null  object \n",
      " 14  heel_shape_type     0 non-null      float64\n",
      " 15  toecap_type         0 non-null      float64\n",
      " 16  woven_structure     62510 non-null  object \n",
      " 17  knit_structure      16926 non-null  object \n",
      " 18  print_type          95239 non-null  object \n",
      " 19  archetype           55607 non-null  object \n",
      " 20  moment              95339 non-null  object \n",
      " 21  phase_in            95339 non-null  object \n",
      " 22  phase_out           95339 non-null  object \n",
      " 23  life_cycle_length   95339 non-null  int64  \n",
      " 24  num_stores          95339 non-null  int64  \n",
      " 25  num_sizes           95339 non-null  int64  \n",
      " 26  has_plus_sizes      95339 non-null  bool   \n",
      " 27  price               95339 non-null  float64\n",
      " 28  year                95339 non-null  int64  \n",
      " 29  num_week_iso        95339 non-null  int64  \n",
      " 30  weekly_sales        95339 non-null  int64  \n",
      " 31  weekly_demand       95339 non-null  int64  \n",
      " 32  Production          95339 non-null  int64  \n",
      "dtypes: bool(1), float64(3), int64(10), object(19)\n",
      "memory usage: 23.4+ MB\n",
      "\n",
      "\n",
      "--- Información de test_df (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2250 entries, 0 to 2249\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  2250 non-null   int64  \n",
      " 1   id_season           2250 non-null   int64  \n",
      " 2   aggregated_family   2250 non-null   object \n",
      " 3   family              2250 non-null   object \n",
      " 4   category            2250 non-null   object \n",
      " 5   fabric              2250 non-null   object \n",
      " 6   color_name          2250 non-null   object \n",
      " 7   color_rgb           2250 non-null   object \n",
      " 8   image_embedding     2250 non-null   object \n",
      " 9   length_type         1868 non-null   object \n",
      " 10  silhouette_type     1771 non-null   object \n",
      " 11  waist_type          450 non-null    object \n",
      " 12  neck_lapel_type     1387 non-null   object \n",
      " 13  sleeve_length_type  1336 non-null   object \n",
      " 14  heel_shape_type     0 non-null      float64\n",
      " 15  toecap_type         0 non-null      float64\n",
      " 16  woven_structure     1382 non-null   object \n",
      " 17  knit_structure      395 non-null    object \n",
      " 18  print_type          2247 non-null   object \n",
      " 19  archetype           200 non-null    object \n",
      " 20  moment              2250 non-null   object \n",
      " 21  phase_in            2250 non-null   object \n",
      " 22  phase_out           2250 non-null   object \n",
      " 23  life_cycle_length   2250 non-null   int64  \n",
      " 24  num_stores          2250 non-null   int64  \n",
      " 25  num_sizes           2250 non-null   int64  \n",
      " 26  has_plus_sizes      2250 non-null   bool   \n",
      " 27  price               2250 non-null   float64\n",
      " 28  Unnamed: 28         0 non-null      float64\n",
      " 29  Unnamed: 29         0 non-null      float64\n",
      " 30  Unnamed: 30         0 non-null      float64\n",
      " 31  Unnamed: 31         0 non-null      float64\n",
      " 32  Unnamed: 32         0 non-null      float64\n",
      "dtypes: bool(1), float64(8), int64(5), object(19)\n",
      "memory usage: 564.8+ KB\n",
      "\n",
      "\n",
      "--- Conteo de filas por 'id' en train_df ---\n",
      "ID\n",
      "983      31\n",
      "2520     31\n",
      "11792    31\n",
      "7845     31\n",
      "6056     31\n",
      "12543    30\n",
      "5100     30\n",
      "3596     30\n",
      "3476     30\n",
      "6439     30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- Conteo de filas por 'id' en test_df ---\n",
      "ID\n",
      "90     1\n",
      "16     1\n",
      "65     1\n",
      "138    1\n",
      "166    1\n",
      "252    1\n",
      "234    1\n",
      "306    1\n",
      "274    1\n",
      "268    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 1: Tarea 2 (Inspección Rápida) ---\n",
    "\n",
    "print(\"--- Vistazo a train_df (head) ---\")\n",
    "print(train_df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Información de train_df (.info()) ---\")\n",
    "# Esto nos dirá los tipos de datos (Dtypes) y los nulos\n",
    "train_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Información de test_df (.info()) ---\")\n",
    "test_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Paso 1: Tarea 3 (Validar Hipótesis de Agregación) ---\n",
    "\n",
    "print(\"--- Conteo de filas por 'id' en train_df ---\")\n",
    "# Si vemos números mayores a 1, significa que hay varias filas por id (semanal)\n",
    "print(train_df['ID'].value_counts().head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Conteo de filas por 'id' en test_df ---\")\n",
    "# Aquí deberíamos ver que todos los valores son '1'\n",
    "print(test_df['ID'].value_counts().head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6b6eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando 'weekly_demand' para crear el target (y_train_agg)...\n",
      "Target (y_train_agg) creado. Ejemplo:\n",
      "ID\n",
      "1      806\n",
      "2     2266\n",
      "3    63791\n",
      "4    11004\n",
      "6    14684\n",
      "Name: weekly_demand, dtype: int64\n",
      "\n",
      "\n",
      "Agregando features (X_train_agg)...\n",
      "Encontradas 27 columnas en común.\n",
      "\n",
      "¡Agregación completada y alineada con éxito!\n",
      "Forma de X_train_agg: (9843, 27)\n",
      "Forma de y_train_agg: (9843,)\n",
      "Forma de test_df_clean: (2250, 27)\n",
      "\n",
      "\n",
      "--- Columnas de X_train_agg (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9843 entries, 1 to 12767\n",
      "Data columns (total 27 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   moment              9843 non-null   object \n",
      " 1   archetype           5663 non-null   object \n",
      " 2   phase_in            9843 non-null   object \n",
      " 3   neck_lapel_type     6407 non-null   object \n",
      " 4   phase_out           9843 non-null   object \n",
      " 5   color_rgb           9843 non-null   object \n",
      " 6   print_type          9838 non-null   object \n",
      " 7   num_stores          9843 non-null   int64  \n",
      " 8   heel_shape_type     0 non-null      float64\n",
      " 9   has_plus_sizes      9843 non-null   bool   \n",
      " 10  knit_structure      1965 non-null   object \n",
      " 11  waist_type          2140 non-null   object \n",
      " 12  price               9843 non-null   float64\n",
      " 13  silhouette_type     8638 non-null   object \n",
      " 14  toecap_type         0 non-null      float64\n",
      " 15  family              9843 non-null   object \n",
      " 16  life_cycle_length   9843 non-null   int64  \n",
      " 17  length_type         9046 non-null   object \n",
      " 18  image_embedding     9843 non-null   object \n",
      " 19  color_name          9843 non-null   object \n",
      " 20  category            9843 non-null   object \n",
      " 21  woven_structure     6355 non-null   object \n",
      " 22  id_season           9843 non-null   int64  \n",
      " 23  aggregated_family   9843 non-null   object \n",
      " 24  sleeve_length_type  6271 non-null   object \n",
      " 25  fabric              9843 non-null   object \n",
      " 26  num_sizes           9843 non-null   int64  \n",
      "dtypes: bool(1), float64(3), int64(4), object(19)\n",
      "memory usage: 2.0+ MB\n",
      "\n",
      "\n",
      "--- Columnas de test_df_clean (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2250 entries, 0 to 2249\n",
      "Data columns (total 27 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   moment              2250 non-null   object \n",
      " 1   archetype           200 non-null    object \n",
      " 2   phase_in            2250 non-null   object \n",
      " 3   neck_lapel_type     1387 non-null   object \n",
      " 4   phase_out           2250 non-null   object \n",
      " 5   color_rgb           2250 non-null   object \n",
      " 6   print_type          2247 non-null   object \n",
      " 7   num_stores          2250 non-null   int64  \n",
      " 8   heel_shape_type     0 non-null      float64\n",
      " 9   has_plus_sizes      2250 non-null   bool   \n",
      " 10  knit_structure      395 non-null    object \n",
      " 11  waist_type          450 non-null    object \n",
      " 12  price               2250 non-null   float64\n",
      " 13  silhouette_type     1771 non-null   object \n",
      " 14  toecap_type         0 non-null      float64\n",
      " 15  family              2250 non-null   object \n",
      " 16  life_cycle_length   2250 non-null   int64  \n",
      " 17  length_type         1868 non-null   object \n",
      " 18  image_embedding     2250 non-null   object \n",
      " 19  color_name          2250 non-null   object \n",
      " 20  category            2250 non-null   object \n",
      " 21  woven_structure     1382 non-null   object \n",
      " 22  id_season           2250 non-null   int64  \n",
      " 23  aggregated_family   2250 non-null   object \n",
      " 24  sleeve_length_type  1336 non-null   object \n",
      " 25  fabric              2250 non-null   object \n",
      " 26  num_sizes           2250 non-null   int64  \n",
      "dtypes: bool(1), float64(3), int64(4), object(19)\n",
      "memory usage: 459.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- [INICIO] Código Corregido para Paso 2 ---\n",
    "\n",
    "# --- Paso 2: Tarea 1 (Crear el Target 'y_train_agg') ---\n",
    "print(\"Agregando 'weekly_demand' para crear el target (y_train_agg)...\")\n",
    "\n",
    "# (CORRECCIÓN): Usamos 'ID' (mayúscula) como vimos en tu CSV\n",
    "y_train_agg = train_df.groupby('ID')['weekly_demand'].sum()\n",
    "\n",
    "print(\"Target (y_train_agg) creado. Ejemplo:\")\n",
    "print(y_train_agg.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Paso 2: Tarea 2 (Crear las Features 'X_train_agg') ---\n",
    "print(\"Agregando features (X_train_agg)...\")\n",
    "\n",
    "# (CORRECCIÓN): Usamos 'ID' (mayúscula)\n",
    "X_train_agg_full = train_df.groupby('ID').first()\n",
    "\n",
    "\n",
    "# --- Tarea 2b: Alinear Columnas (¡NUEVA LÓGICA MÁS ROBUSTA!) ---\n",
    "# En lugar de usar la lista 'test_df.columns' (que tiene 'ID' y 'Unnamed'),\n",
    "# vamos a buscar las columnas que SÍ están en ambos sitios.\n",
    "\n",
    "train_features = set(X_train_agg_full.columns)\n",
    "test_features = set(test_df.columns)\n",
    "\n",
    "# Buscamos la intersección: columnas que están en X_train_agg_full Y en test_df\n",
    "# Esto excluye automáticamente 'ID' (que no es columna en train) \n",
    "# y 'Unnamed:' (que no están en train)\n",
    "common_columns = list(train_features.intersection(test_features))\n",
    "\n",
    "print(f\"Encontradas {len(common_columns)} columnas en común.\")\n",
    "\n",
    "# Ahora filtramos ambos DataFrames para que SÓLO tengan estas columnas\n",
    "X_train_agg = X_train_agg_full[common_columns].copy() # Usamos .copy() para evitar warnings\n",
    "\n",
    "# ¡Importante! También limpiamos test_df para que coincida\n",
    "# (Guardamos el 'ID' original de test_df para la sumisión final)\n",
    "test_ids_for_submission = test_df['ID']\n",
    "test_df_clean = test_df[common_columns].copy() \n",
    "\n",
    "\n",
    "# --- Tarea 2c: Alineación Final ---\n",
    "# Nos aseguramos de que 'y_train_agg' siga el mismo orden que 'X_train_agg'\n",
    "y_train_agg = y_train_agg.reindex(X_train_agg.index)\n",
    "\n",
    "print(\"\\n¡Agregación completada y alineada con éxito!\")\n",
    "print(f\"Forma de X_train_agg: {X_train_agg.shape}\")\n",
    "print(f\"Forma de y_train_agg: {y_train_agg.shape}\")\n",
    "print(f\"Forma de test_df_clean: {test_df_clean.shape}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Columnas de X_train_agg (.info()) ---\")\n",
    "X_train_agg.info()\n",
    "print(\"\\n\")\n",
    "print(\"--- Columnas de test_df_clean (.info()) ---\")\n",
    "test_df_clean.info()\n",
    "\n",
    "# --- [FIN] Código Corregido para Paso 2 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "608a26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 3: Ingeniería de Características...\n",
      "Feature 'start_month' creada con éxito. Ejemplo:\n",
      "     phase_in  start_month\n",
      "ID                        \n",
      "1  2023-02-01          2.0\n",
      "2         NaT          6.0\n",
      "3         NaT          6.0\n",
      "4         NaT          6.0\n",
      "6         NaT          6.0\n",
      "\n",
      "\n",
      "Definidas 5 features numéricas:\n",
      "['num_stores', 'price', 'life_cycle_length', 'num_sizes', 'start_month']\n",
      "\n",
      "\n",
      "Definidas 17 features categóricas:\n",
      "['moment', 'archetype', 'neck_lapel_type', 'print_type', 'has_plus_sizes', 'knit_structure', 'waist_type', 'silhouette_type', 'family', 'length_type', 'color_name', 'category', 'woven_structure', 'id_season', 'aggregated_family', 'sleeve_length_type', 'fabric']\n",
      "\n",
      "¡Paso 3 completado! Estamos listos para construir el Pipeline.\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Ingeniería de Características ---\n",
    "\n",
    "print(\"Iniciando Paso 3: Ingeniería de Características...\")\n",
    "\n",
    "# Hacemos una copia para evitar 'SettingWithCopyWarning'\n",
    "X_train_features = X_train_agg.copy()\n",
    "X_test_features = test_df_clean.copy()\n",
    "\n",
    "# --- Tarea 1: Crear 'start_month' (Ingeniería de Fechas) ---\n",
    "# Convertimos 'phase_in' a formato fecha (MANDATORIO para extraer el mes)\n",
    "# errors='coerce' convertirá cualquier fecha inválida en NaT (Nulo)\n",
    "X_train_features['phase_in'] = pd.to_datetime(X_train_features['phase_in'], errors='coerce')\n",
    "X_test_features['phase_in'] = pd.to_datetime(X_test_features['phase_in'], errors='coerce')\n",
    "\n",
    "# Extraemos el mes (1-12) como una nueva feature numérica\n",
    "X_train_features['start_month'] = X_train_features['phase_in'].dt.month\n",
    "X_test_features['start_month'] = X_test_features['phase_in'].dt.month\n",
    "\n",
    "# Ahora rellenamos cualquier nulo que se haya creado (por si 'errors=coerce' falló)\n",
    "# Usaremos la mediana (ej. 6, para Junio/Julio)\n",
    "median_month = X_train_features['start_month'].median()\n",
    "X_train_features['start_month'] = X_train_features['start_month'].fillna(median_month)\n",
    "X_test_features['start_month'] = X_test_features['start_month'].fillna(median_month)\n",
    "\n",
    "print(\"Feature 'start_month' creada con éxito. Ejemplo:\")\n",
    "print(X_train_features[['phase_in', 'start_month']].head())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Tarea 2: Definir Listas de Columnas para el Pipeline ---\n",
    "# Basado en nuestro análisis del .info()\n",
    "\n",
    "# 1. Columnas Numéricas: Rellenaremos nulos con la mediana y las escalaremos\n",
    "numeric_features = [\n",
    "    'num_stores',\n",
    "    'price',\n",
    "    'life_cycle_length',\n",
    "    'num_sizes',\n",
    "    'start_month'  # Nuestra nueva feature\n",
    "]\n",
    "\n",
    "# 2. Columnas Categóricas: Rellenaremos nulos con \"Desconocido\" y aplicaremos One-Hot Encoding\n",
    "categorical_features = [\n",
    "    'moment',\n",
    "    'archetype',\n",
    "    'neck_lapel_type',\n",
    "    'print_type',\n",
    "    'has_plus_sizes',       # Aunque es bool, lo tratamos como categórico\n",
    "    'knit_structure',\n",
    "    'waist_type',\n",
    "    'silhouette_type',\n",
    "    'family',\n",
    "    'length_type',\n",
    "    'color_name',\n",
    "    'category',\n",
    "    'woven_structure',\n",
    "    'id_season',            # Aunque es int, es una CATEGORÍA (ej. temporada 86)\n",
    "    'aggregated_family',\n",
    "    'sleeve_length_type',\n",
    "    'fabric'\n",
    "]\n",
    "\n",
    "# 3. Columnas a Eliminar: No las usaremos\n",
    "# (El Pipeline se encargará de esto automáticamente al NO incluirlas \n",
    "# en las listas de arriba, excepto las que creamos y ya no necesitamos)\n",
    "\n",
    "print(f\"Definidas {len(numeric_features)} features numéricas:\")\n",
    "print(numeric_features)\n",
    "print(\"\\n\")\n",
    "print(f\"Definidas {len(categorical_features)} features categóricas:\")\n",
    "print(categorical_features)\n",
    "print(\"\\n¡Paso 3 completado! Estamos listos para construir el Pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f01e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas de Scikit-learn importadas.\n",
      "Pipelines de transformación numérica y categórica definidos.\n",
      "ColumnTransformer ('preprocessor') creado con éxito.\n",
      "\n",
      "¡Paso 4 completado! El 'preprocessor' está listo para ser usado.\n",
      "\n",
      "--- Verificación del Preprocesador ---\n",
      "Forma original de X_train: (9843, 28)\n",
      "Forma de X_train procesado: (9843, 297)\n",
      "¡El número de columnas ha crecido por el OneHotEncoding!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 4: Preprocesamiento y Pipeline de Scikit-learn ---\n",
    "\n",
    "# Importamos todas las herramientas que necesitamos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "print(\"Herramientas de Scikit-learn importadas.\")\n",
    "\n",
    "# --- Tarea 1: Definir los Mini-Pipelines ---\n",
    "\n",
    "# Pipeline para datos NUMÉRICOS\n",
    "# 1. SimpleImputer: Rellena cualquier nulo (ej. en 'start_month') con la mediana.\n",
    "# 2. StandardScaler: Escala los datos (pone todo ~ entre -2 y 2). \n",
    "#    RandomForest no lo necesita, pero es BUENA PRÁCTICA para otros modelos.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para datos CATEGÓRICOS\n",
    "# 1. SimpleImputer: Rellena los nulos (ej. en 'archetype') con la palabra \"Desconocido\".\n",
    "# 2. OneHotEncoder: Crea nuevas columnas para cada categoría (ej. family_Dresses, family_Coats).\n",
    "#    handle_unknown='ignore' -> Si en test_df aparece un color que no vio en train,\n",
    "#    simplemente lo ignora sin dar error. ¡Vital!\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "print(\"Pipelines de transformación numérica y categórica definidos.\")\n",
    "\n",
    "# --- Tarea 2: Unir con ColumnTransformer ---\n",
    "# Aquí es donde le decimos a sklearn:\n",
    "# - Aplica 'numeric_transformer' a las columnas de 'numeric_features'\n",
    "# - Aplica 'categorical_transformer' a las columnas de 'categorical_features'\n",
    "# - remainder='drop' -> Todas las columnas que no mencionamos (ej. image_embedding) serán ELIMINADAS.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Ignora las columnas que no están en nuestras listas\n",
    ")\n",
    "\n",
    "print(\"ColumnTransformer ('preprocessor') creado con éxito.\")\n",
    "print(\"\\n¡Paso 4 completado! El 'preprocessor' está listo para ser usado.\")\n",
    "\n",
    "# --- Bonus: Verifiquemos qué ha creado ---\n",
    "# Vamos a \"entrenar\" solo el preprocesador y ver la forma de la salida\n",
    "# Usamos las variables que creamos en el Paso 3\n",
    "\n",
    "# Renombramos por simplicidad\n",
    "X = X_train_features\n",
    "y = y_train_agg\n",
    "X_test = X_test_features\n",
    "\n",
    "# 'fit_transform' aprende de X (ej. calcula la mediana) y luego transforma X\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- Verificación del Preprocesador ---\")\n",
    "print(f\"Forma original de X_train: {X.shape}\")\n",
    "print(f\"Forma de X_train procesado: {X_processed.shape}\")\n",
    "print(\"¡El número de columnas ha crecido por el OneHotEncoding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a4a21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas de modelo y métricas importadas.\n",
      "Datos divididos: 7874 para entrenar, 1969 para validar.\n",
      "\n",
      "\n",
      "Pipeline final creado (Preprocesador + RandomForest).\n",
      "Entrenando el modelo... (Esto puede tardar 1-2 minutos)\n",
      "¡Modelo entrenado con éxito!\n",
      "\n",
      "\n",
      "--- Evaluación del Modelo (sobre datos de validación) ---\n",
      "Error Absoluto Medio (MAE): 3822.0330\n",
      "Raíz del Error Cuadrático Medio (RMSE): 7800.2950\n",
      "\n",
      "\n",
      "--- Interpretación ---\n",
      "Un MAE de 3822.0330 significa que, en promedio, las predicciones de demanda total del modelo\n",
      "tienen un error de +/- 3822.0330 unidades (en la escala 0-1).\n",
      "\n",
      "¡Paso 5 y 6 completados!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 5: Entrenamiento del Modelo ---\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"Herramientas de modelo y métricas importadas.\")\n",
    "\n",
    "# --- Tarea 1: Separar Datos de Validación ---\n",
    "# Dividimos nuestros datos (X, y) en 80% para entrenar y 20% para validar\n",
    "# random_state=42 asegura que la división sea siempre la misma\n",
    "X_train_local, X_val_local, y_train_local, y_val_local = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Datos divididos: {len(X_train_local)} para entrenar, {len(X_val_local)} para validar.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Tarea 2: Crear el Pipeline Final ---\n",
    "# Unimos el 'preprocessor' (Paso 4) y nuestro modelo 'RandomForestRegressor'\n",
    "\n",
    "# n_estimators=100 -> Crear un bosque de 100 árboles (es un buen número para empezar)\n",
    "# n_jobs=-1 -> Usar todos los núcleos de tu CPU para entrenar más rápido\n",
    "# random_state=42 -> Para que el entrenamiento sea reproducible\n",
    "model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "print(\"Pipeline final creado (Preprocesador + RandomForest).\")\n",
    "\n",
    "# --- Tarea 3: Entrenar ---\n",
    "print(\"Entrenando el modelo... (Esto puede tardar 1-2 minutos)\")\n",
    "# ¡Aquí ocurre la magia!\n",
    "# .fit() entrena el preprocesador y el modelo, todo en una línea.\n",
    "full_pipeline.fit(X_train_local, y_train_local)\n",
    "\n",
    "print(\"¡Modelo entrenado con éxito!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Paso 6: Evaluación y Ajuste ---\n",
    "\n",
    "print(\"--- Evaluación del Modelo (sobre datos de validación) ---\")\n",
    "\n",
    "# 1. Predecir sobre los datos de validación\n",
    "y_pred_val = full_pipeline.predict(X_val_local)\n",
    "\n",
    "# 2. Calcular Métricas de Error\n",
    "mae = mean_absolute_error(y_val_local, y_pred_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_local, y_pred_val))\n",
    "\n",
    "print(f\"Error Absoluto Medio (MAE): {mae:.4f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Interpretación ---\")\n",
    "print(f\"Un MAE de {mae:.4f} significa que, en promedio, las predicciones de demanda total del modelo\")\n",
    "print(f\"tienen un error de +/- {mae:.4f} unidades (en la escala 0-1).\")\n",
    "print(\"\\n¡Paso 5 y 6 completados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7715f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 7: Predicción Final...\n",
      "Re-entrenando el modelo con el 100% de los datos de entrenamiento...\n",
      "¡Modelo final entrenado!\n",
      "Generando predicciones sobre los datos de test...\n",
      "¡Predicciones generadas!\n",
      "Predicciones negativas ajustadas a 0.\n",
      "\n",
      "\n",
      "==================================================\n",
      "¡Archivo 'submission2.csv' creado con éxito!\n",
      "==================================================\n",
      "Vistazo al archivo de envío:\n",
      "    ID   demand\n",
      "0   90  1272.69\n",
      "1   16  6059.90\n",
      "2   65  9059.40\n",
      "3  138   820.80\n",
      "4  166   328.27\n",
      "\n",
      "\n",
      "¡Paso 7 completado!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 7: Predicción Final y Archivo de Envío ---\n",
    "\n",
    "print(\"Iniciando Paso 7: Predicción Final...\")\n",
    "\n",
    "# --- Tarea 1: Entrenar con TODOS los datos ---\n",
    "print(\"Re-entrenando el modelo con el 100% de los datos de entrenamiento...\")\n",
    "# Usamos 'X' e 'y' (los DataFrames completos que creamos en el Paso 3)\n",
    "# 'X_test_features' es el set de test limpio\n",
    "full_pipeline.fit(X, y)\n",
    "\n",
    "print(\"¡Modelo final entrenado!\")\n",
    "\n",
    "# --- Tarea 2: Predecir sobre test_df_clean ---\n",
    "print(\"Generando predicciones sobre los datos de test...\")\n",
    "# (Recuerda que X_test_features es el nombre que le dimos a test_df_clean en el Paso 3)\n",
    "final_predictions = full_pipeline.predict(X_test_features)\n",
    "\n",
    "print(\"¡Predicciones generadas!\")\n",
    "\n",
    "# --- Tarea 3: Post-Procesamiento (¡MUY IMPORTANTE!) ---\n",
    "# La demanda no puede ser negativa.\n",
    "# Reemplazamos cualquier predicción negativa por 0.\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "print(\"Predicciones negativas ajustadas a 0.\")\n",
    "\n",
    "# --- Tarea 4: Crear Archivo de Envío ---\n",
    "# Usamos los 'test_ids_for_submission' que guardamos en el Paso 2\n",
    "# y nuestras 'final_predictions'\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids_for_submission,\n",
    "    'demand': final_predictions\n",
    "})\n",
    "\n",
    "# --- Tarea 5: Guardar el Archivo ---\n",
    "submission_filename = 'submission2.csv'\n",
    "submission_df.to_csv(submission_filename, index=False, sep=',')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"¡Archivo '{submission_filename}' creado con éxito!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Vistazo al archivo de envío:\")\n",
    "print(submission_df.head())\n",
    "print(\"\\n\")\n",
    "print(\"¡Paso 7 completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76c193f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 'EmbeddingSplitter' definida.\n",
      "Pipelines de transformación definidos (incluyendo embeddings).\n",
      "ColumnTransformer ('preprocessor_v2') creado con éxito.\n",
      "\n",
      "¡Paso 4 (V2) completado! El preprocesador ahora usa PCA en los embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 4 (Versión 2): Preprocesador Avanzado ---\n",
    "\n",
    "# Nuevas importaciones\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- Tarea 1: Transformador de Embeddings a Medida ---\n",
    "# Esta clase enseñará a scikit-learn a manejar esos strings de embeddings\n",
    "class EmbeddingSplitter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Toma una Serie de strings (ej. \"0.1,0.2,-0.5\"), \n",
    "    los separa por ',' y los devuelve como un array numérico.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # X es una Serie de pandas, ej. X_train_features['image_embedding']\n",
    "        # 1. Aplicamos .str.split() para separar por comas\n",
    "        # 2. 'expand=True' lo convierte en un DataFrame\n",
    "        # 3. .astype(float) lo convierte en números\n",
    "        # 4. .fillna(0) es una seguridad por si hay nulos\n",
    "        return X.str.split(',', expand=True).astype(float).fillna(0)\n",
    "\n",
    "print(\"Clase 'EmbeddingSplitter' definida.\")\n",
    "\n",
    "\n",
    "# --- Tarea 2: Definir los Mini-Pipelines (V2) ---\n",
    "\n",
    "# Pipeline Numérico (igual que antes)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline Categórico (igual que antes)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# --- (NUEVO) Pipeline de Embeddings ---\n",
    "# 1. Llama a nuestro 'EmbeddingSplitter' para obtener las 512+ columnas\n",
    "# 2. Llama a 'PCA' para comprimir esas 512+ columnas en solo 10\n",
    "embedding_transformer = Pipeline(steps=[\n",
    "    ('split', EmbeddingSplitter()),\n",
    "    ('pca', PCA(n_components=10, random_state=42)) # Comprimimos a 10 \"super-features\"\n",
    "])\n",
    "\n",
    "print(\"Pipelines de transformación definidos (incluyendo embeddings).\")\n",
    "\n",
    "# --- Tarea 3: Unir con ColumnTransformer (V2) ---\n",
    "# Ahora le pasamos TRES transformadores a nuestro preprocesador\n",
    "\n",
    "preprocessor_v2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        # (NUEVO) Aplicamos el pipeline de embeddings a la columna 'image_embedding'\n",
    "        ('embed', embedding_transformer, 'image_embedding')\n",
    "    ],\n",
    "    remainder='drop' # Eliminamos las que no usamos (ej. color_rgb, phase_in)\n",
    ")\n",
    "\n",
    "print(\"ColumnTransformer ('preprocessor_v2') creado con éxito.\")\n",
    "print(\"\\n¡Paso 4 (V2) completado! El preprocesador ahora usa PCA en los embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cf7f3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline final (V2) creado (Preprocesador Avanzado + LGBM).\n",
      "Entrenando el modelo LGBM... (Esto suele ser más rápido que RandomForest)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3297\n",
      "[LightGBM] [Info] Number of data points in the train set: 7874, number of used features: 216\n",
      "[LightGBM] [Info] Start training from score 5162.500000\n",
      "¡Modelo LGBM entrenado con éxito!\n",
      "\n",
      "\n",
      "--- Evaluación del Modelo LGBM (sobre datos de validación) ---\n",
      "--- RESULTADOS (Versión 2) ---\n",
      "Error Absoluto Medio (MAE): 3798.7870\n",
      "Raíz del Error Cuadrático Medio (RMSE): 7392.0655\n",
      "\n",
      "\n",
      "--- Comparación ---\n",
      "MAE Anterior (RandomForest): 3822.0330\n",
      "MAE Nuevo (LGBM + Embeddings): 3798.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 5 y 6 (Versión 2): Entrenamiento y Evaluación con LGBM ---\n",
    "\n",
    "# 1. Definir el modelo LGBM\n",
    "#    objective='regression_l1' -> Le decimos que optimice para MAE (nuestra métrica)\n",
    "#    n_estimators=300 -> Más árboles, ya que es más rápido que RandomForest\n",
    "model_lgbm = LGBMRegressor(\n",
    "    objective='regression_l1', # Optimiza para MAE\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# 2. Crear el Pipeline Final (V2)\n",
    "full_pipeline_v2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('model', model_lgbm)\n",
    "])\n",
    "\n",
    "print(\"Pipeline final (V2) creado (Preprocesador Avanzado + LGBM).\")\n",
    "\n",
    "# 3. Entrenar (usamos los mismos datos de validación de antes)\n",
    "print(\"Entrenando el modelo LGBM... (Esto suele ser más rápido que RandomForest)\")\n",
    "full_pipeline_v2.fit(X_train_local, y_train_local)\n",
    "\n",
    "print(\"¡Modelo LGBM entrenado con éxito!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Evaluar (V2)\n",
    "print(\"--- Evaluación del Modelo LGBM (sobre datos de validación) ---\")\n",
    "y_pred_val_v2 = full_pipeline_v2.predict(X_val_local)\n",
    "\n",
    "# 5. Calcular Métricas de Error (V2)\n",
    "mae_v2 = mean_absolute_error(y_val_local, y_pred_val_v2)\n",
    "rmse_v2 = np.sqrt(mean_squared_error(y_val_local, y_pred_val_v2))\n",
    "\n",
    "print(\"--- RESULTADOS (Versión 2) ---\")\n",
    "print(f\"Error Absoluto Medio (MAE): {mae_v2:.4f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse_v2:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(\"--- Comparación ---\")\n",
    "print(f\"MAE Anterior (RandomForest): {mae:.4f}\")\n",
    "print(f\"MAE Nuevo (LGBM + Embeddings): {mae_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fde82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando V3: Probando el poder de la transformación Logarítmica...\n",
      "Target 'y' transformado a logaritmo (y_train_log).\n",
      "Ejemplo de 'y' original: [  806  2266 63791 11004 14684]\n",
      "Ejemplo de 'y' log-transformado: [ 6.69332367  7.72621265 11.06338307  9.30610499  9.59458184]\n",
      "\n",
      "\n",
      "Datos divididos (X y y_log).\n",
      "Entrenando el RandomForest (V1) en el target logarítmico...\n",
      "¡Modelo V3 (RF + Log) entrenado!\n",
      "\n",
      "\n",
      "--- Evaluación del Modelo V3 ---\n",
      "--- RESULTADOS (Versión 3) ---\n",
      "Error Absoluto Medio (MAE): 3752.6822\n",
      "Raíz del Error Cuadrático Medio (RMSE): 7702.0142\n",
      "\n",
      "\n",
      "--- Comparación ---\n",
      "MAE V1 (RF simple): 3822.0330\n",
      "MAE V2 (LGBM + Embeds): 3798.7870\n",
      "MAE V3 (RF + Log-Transform): 3752.6822\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 5 y 6 (Versión 3): Entrenamiento con Log-Transform ---\n",
    "\n",
    "print(\"Iniciando V3: Probando el poder de la transformación Logarítmica...\")\n",
    "\n",
    "# --- Tarea 1: Transformar el Target (y) ---\n",
    "# Usamos np.log1p, que es log(x + 1). \n",
    "# Es más seguro que log(x) por si algún valor es 0.\n",
    "y_train_log = np.log1p(y_train_agg)\n",
    "\n",
    "print(\"Target 'y' transformado a logaritmo (y_train_log).\")\n",
    "print(\"Ejemplo de 'y' original:\", y_train_agg.head().values)\n",
    "print(\"Ejemplo de 'y' log-transformado:\", y_train_log.head().values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Tarea 2: Dividir los datos (X y el nuevo y_train_log) ---\n",
    "# Usamos los mismos X_train_local y X_val_local de antes\n",
    "# Pero dividimos el y_train_log\n",
    "y_train_local_log, y_val_local_log = train_test_split(\n",
    "    y_train_log, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "# Nota: 'y_val_local' (el original, sin log) lo guardamos para la comparación final\n",
    "\n",
    "print(\"Datos divididos (X y y_log).\")\n",
    "\n",
    "# --- Tarea 3: Re-crear y Entrenar el Pipeline V1 (RandomForest simple) ---\n",
    "# (Usamos el 'preprocessor' simple, el primero que hicimos SIN embeddings)\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Usamos el 'preprocessor' original (Paso 4, V1)\n",
    "full_pipeline_v1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('model', model_rf)\n",
    "])\n",
    "\n",
    "print(\"Entrenando el RandomForest (V1) en el target logarítmico...\")\n",
    "# Entrenamos 'X' para predecir 'y_log'\n",
    "full_pipeline_v1.fit(X_train_local, y_train_local_log)\n",
    "\n",
    "print(\"¡Modelo V3 (RF + Log) entrenado!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Tarea 4: Evaluar (V3) ---\n",
    "print(\"--- Evaluación del Modelo V3 ---\")\n",
    "\n",
    "# 1. Predecir sobre X_val_local. Las predicciones estarán en escala LOG\n",
    "y_pred_val_log = full_pipeline_v1.predict(X_val_local)\n",
    "\n",
    "# 2. ¡CRÍTICO! Invertir la transformación\n",
    "# Usamos np.expm1, que es exp(x) - 1 (la inversa de log1p)\n",
    "y_pred_val_real = np.expm1(y_pred_val_log)\n",
    "\n",
    "# 3. Calcular Métricas de Error\n",
    "# Comparamos los valores REALES (y_val_local) con nuestras predicciones REALES (y_pred_val_real)\n",
    "mae_v3 = mean_absolute_error(y_val_local, y_pred_val_real)\n",
    "rmse_v3 = np.sqrt(mean_squared_error(y_val_local, y_pred_val_real))\n",
    "\n",
    "print(\"--- RESULTADOS (Versión 3) ---\")\n",
    "print(f\"Error Absoluto Medio (MAE): {mae_v3:.4f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse_v3:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(\"--- Comparación ---\")\n",
    "print(f\"MAE V1 (RF simple): {mae:.4f}\")\n",
    "print(f\"MAE V2 (LGBM + Embeds): {mae_v2:.4f}\")\n",
    "print(f\"MAE V3 (RF + Log-Transform): {mae_v3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7531b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelines de transformación (V3) definidos.\n",
      "ColumnTransformer ('preprocessor_v3') creado con éxito.\n",
      "¡El preprocesador 'All-In' está listo!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 4 (Versión 3): Preprocesador \"All-In\" ---\n",
    "\n",
    "# Pipeline Numérico (igual)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline Categórico (igual)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# --- (NUEVO) Pipeline de Embeddings (Corregido) ---\n",
    "# 1. Llama a nuestro 'EmbeddingSplitter'\n",
    "# 2. Llama a 'PCA' para comprimir a 64 componentes (en lugar de 10)\n",
    "embedding_transformer_v3 = Pipeline(steps=[\n",
    "    ('split', EmbeddingSplitter()),\n",
    "    ('pca', PCA(n_components=64, random_state=42)) # <-- CORRECCIÓN: 64 COMPONENTES\n",
    "])\n",
    "\n",
    "print(\"Pipelines de transformación (V3) definidos.\")\n",
    "\n",
    "# --- Tarea 3: Unir con ColumnTransformer (V3) ---\n",
    "preprocessor_v3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('embed', embedding_transformer_v3, 'image_embedding') # Usamos el nuevo PCA\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"ColumnTransformer ('preprocessor_v3') creado con éxito.\")\n",
    "print(\"¡El preprocesador 'All-In' está listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ef6756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline final (V4) creado (Preprocesador V3 + LGBM).\n",
      "Entrenando el modelo 'All-In' (LGBM + Log + PCA_64)...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17067\n",
      "[LightGBM] [Info] Number of data points in the train set: 7874, number of used features: 270\n",
      "[LightGBM] [Info] Start training from score 8.549370\n",
      "¡Modelo V4 entrenado con éxito!\n",
      "\n",
      "\n",
      "--- Evaluación del Modelo V4 ---\n",
      "--- RESULTADOS (Versión 4) ---\n",
      "Error Absoluto Medio (MAE): 3794.8554\n",
      "Raíz del Error Cuadrático Medio (RMSE): 7617.0713\n",
      "\n",
      "\n",
      "--- Comparación Final ---\n",
      "MAE V1 (RF simple): 3822.0330\n",
      "MAE V3 (RF + Log): 3752.6822\n",
      "MAE V4 (LGBM + Log + Embeds): 3794.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janvi\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 5 y 6 (Versión 4): Entrenamiento y Evaluación \"All-In\" ---\n",
    "\n",
    "# 1. Definir el modelo LGBM (igual que V2)\n",
    "model_lgbm = LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# 2. Crear el Pipeline Final (V4)\n",
    "full_pipeline_v4 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_v3), # <-- USAMOS EL NUEVO PREPROCESSOR\n",
    "    ('model', model_lgbm)\n",
    "])\n",
    "\n",
    "print(\"Pipeline final (V4) creado (Preprocesador V3 + LGBM).\")\n",
    "\n",
    "# 3. Entrenar\n",
    "print(\"Entrenando el modelo 'All-In' (LGBM + Log + PCA_64)...\")\n",
    "# Entrenamos 'X_train_local' para predecir 'y_train_local_log'\n",
    "full_pipeline_v4.fit(X_train_local, y_train_local_log)\n",
    "\n",
    "print(\"¡Modelo V4 entrenado con éxito!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Evaluar (V4)\n",
    "print(\"--- Evaluación del Modelo V4 ---\")\n",
    "\n",
    "# 1. Predecir (predicciones en LOG)\n",
    "y_pred_val_log_v4 = full_pipeline_v4.predict(X_val_local)\n",
    "\n",
    "# 2. Invertir la transformación\n",
    "y_pred_val_real_v4 = np.expm1(y_pred_val_log_v4)\n",
    "\n",
    "# 3. Calcular Métricas de Error\n",
    "mae_v4 = mean_absolute_error(y_val_local, y_pred_val_real_v4)\n",
    "rmse_v4 = np.sqrt(mean_squared_error(y_val_local, y_pred_val_real_v4))\n",
    "\n",
    "print(\"--- RESULTADOS (Versión 4) ---\")\n",
    "print(f\"Error Absoluto Medio (MAE): {mae_v4:.4f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse_v4:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(\"--- Comparación Final ---\")\n",
    "print(f\"MAE V1 (RF simple): {mae:.4f}\")\n",
    "print(f\"MAE V3 (RF + Log): {mae_v3:.4f}\")\n",
    "print(f\"MAE V4 (LGBM + Log + Embeds): {mae_v4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f2e1857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-entrenando el mejor modelo (V3: RF + Log) con el 100% de los datos...\n",
      "¡Modelo final (V3) entrenado!\n",
      "Generando predicciones sobre los datos de test...\n",
      "Predicciones invertidas a la escala real.\n",
      "Predicciones 'reales' multiplicadas por el Factor de Seguridad (1.1).\n",
      "\n",
      "\n",
      "==================================================\n",
      "¡Archivo 'submission_v5_strategic.csv' creado con éxito!\n",
      "==================================================\n",
      "Vistazo al archivo de envío (estratégico):\n",
      "    ID       demand\n",
      "0   90  1199.638987\n",
      "1   16  6457.126030\n",
      "2   65  9044.100852\n",
      "3  138   817.957846\n",
      "4  166   344.532367\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 7 (Versión Final): Generando la entrega estratégica ---\n",
    "\n",
    "# --- Tarea 1: Entrenar el Modelo V3 (RF + Log) con TODOS los datos ---\n",
    "print(\"Re-entrenando el mejor modelo (V3: RF + Log) con el 100% de los datos...\")\n",
    "\n",
    "# Usamos 'X' (features completas) e 'y_train_log' (target log-transformado)\n",
    "# Usamos el 'full_pipeline_v1' que contiene el 'preprocessor' simple y el 'RandomForest'\n",
    "full_pipeline_v1.fit(X, y_train_log)\n",
    "\n",
    "print(\"¡Modelo final (V3) entrenado!\")\n",
    "\n",
    "# --- Tarea 2: Predecir sobre Test ---\n",
    "print(\"Generando predicciones sobre los datos de test...\")\n",
    "# (Recuerda que X_test_features es el nombre que le dimos a test_df_clean)\n",
    "predictions_log = full_pipeline_v1.predict(X_test_features)\n",
    "\n",
    "# --- Tarea 3: Invertir la Transformación Logarítmica ---\n",
    "predictions_real = np.expm1(predictions_log)\n",
    "print(\"Predicciones invertidas a la escala real.\")\n",
    "\n",
    "# --- Tarea 4: Post-Procesamiento (Limpieza de Negativos) ---\n",
    "predictions_real[predictions_real < 0] = 0\n",
    "\n",
    "# --- TAREA 5: EL HACK ESTRATÉGICO ---\n",
    "# Basado en el PDF, penalizan más predecir de menos.\n",
    "# Vamos a \"inflar\" todas nuestras predicciones en un 10%\n",
    "# como factor de seguridad contra ventas perdidas.\n",
    "# (Puedes probar 1.05, 1.10, 1.15 en diferentes envíos)\n",
    "\n",
    "SAFETY_FACTOR = 1.10\n",
    "final_predictions_strategic = predictions_real * SAFETY_FACTOR\n",
    "\n",
    "print(f\"Predicciones 'reales' multiplicadas por el Factor de Seguridad ({SAFETY_FACTOR}).\")\n",
    "\n",
    "# --- Tarea 6: Crear Archivo de Envío ---\n",
    "submission_filename = 'submission_v5_strategic.csv'\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids_for_submission,\n",
    "    'demand': final_predictions_strategic\n",
    "})\n",
    "\n",
    "# --- Tarea 7: Guardar el Archivo ---\n",
    "submission_df.to_csv(submission_filename, index=False, sep=',')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"¡Archivo '{submission_filename}' creado con éxito!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Vistazo al archivo de envío (estratégico):\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
