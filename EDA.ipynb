{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b3caaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importació de llibreries bàsiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast  # Per processar els embeddings\n",
    "from pathlib import Path\n",
    "\n",
    "# Preprocessament i Modelització\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43164d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dades carregades:\n",
      "Train:   (95339, 33)\n",
      "Test:    (2250, 33)\n",
      "Sample:  (2250, 2)\n"
     ]
    }
   ],
   "source": [
    "# Càrrega de dades\n",
    "data_dir = Path('.')\n",
    "train_path = data_dir / 'train.csv'\n",
    "test_path = data_dir / 'test.csv'\n",
    "sample_path = data_dir / 'sample_submission.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path, sep=';')\n",
    "    test_df = pd.read_csv(test_path, sep=';')\n",
    "    sample_sub = pd.read_csv(sample_path, sep=',')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Assegura't que els fitxers 'train.csv', 'test.csv' i 'sample_submission.csv' estan al mateix directori.\")\n",
    "\n",
    "print(\"Dades carregades:\")\n",
    "print(f\"Train:   {train_df.shape}\")\n",
    "print(f\"Test:    {test_df.shape}\")\n",
    "print(f\"Sample:  {sample_sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "410ca891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vistazo a train_df (head) ---\n",
      "   ID  id_season      aggregated_family   family  \\\n",
      "0   1         86  Dresses and jumpsuits  Dresses   \n",
      "1   1         86  Dresses and jumpsuits  Dresses   \n",
      "2   1         86  Dresses and jumpsuits  Dresses   \n",
      "3   1         86  Dresses and jumpsuits  Dresses   \n",
      "4   1         86  Dresses and jumpsuits  Dresses   \n",
      "\n",
      "                              category fabric color_name  color_rgb  \\\n",
      "0  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "1  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "2  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "3  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "4  Dresses, jumpsuits and Complete set  WOVEN   AMARILLO  255,215,0   \n",
      "\n",
      "                                     image_embedding length_type  \\\n",
      "0  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "1  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "2  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "3  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "4  0.072266474,-0.12752205,0.6080948,-1.2579741,-...        Long   \n",
      "\n",
      "  silhouette_type waist_type neck_lapel_type sleeve_length_type  \\\n",
      "0           Evase        NaN          V-Neck         Fine Strap   \n",
      "1           Evase        NaN          V-Neck         Fine Strap   \n",
      "2           Evase        NaN          V-Neck         Fine Strap   \n",
      "3           Evase        NaN          V-Neck         Fine Strap   \n",
      "4           Evase        NaN          V-Neck         Fine Strap   \n",
      "\n",
      "   heel_shape_type  toecap_type woven_structure knit_structure     print_type  \\\n",
      "0              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "1              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "2              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "3              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "4              NaN          NaN           Light            NaN  Sin Estampado   \n",
      "\n",
      "  archetype    moment    phase_in   phase_out  life_cycle_length  num_stores  \\\n",
      "0       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "1       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "2       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "3       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "4       NaN  TIME OFF  02/01/2023  26/03/2023                 12         152   \n",
      "\n",
      "   num_sizes  has_plus_sizes  price  year  num_week_iso  weekly_sales  \\\n",
      "0          5           False  35.99  2023             1            66   \n",
      "1          5           False  35.99  2023             2           112   \n",
      "2          5           False  35.99  2023             3           135   \n",
      "3          5           False  35.99  2023             4            99   \n",
      "4          5           False  35.99  2023             5            74   \n",
      "\n",
      "   weekly_demand  Production  \n",
      "0             69        4556  \n",
      "1            112        4556  \n",
      "2            135        4556  \n",
      "3             99        4556  \n",
      "4             74        4556  \n",
      "\n",
      "\n",
      "--- Información de train_df (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95339 entries, 0 to 95338\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  95339 non-null  int64  \n",
      " 1   id_season           95339 non-null  int64  \n",
      " 2   aggregated_family   95339 non-null  object \n",
      " 3   family              95339 non-null  object \n",
      " 4   category            95339 non-null  object \n",
      " 5   fabric              95339 non-null  object \n",
      " 6   color_name          95339 non-null  object \n",
      " 7   color_rgb           95339 non-null  object \n",
      " 8   image_embedding     95339 non-null  object \n",
      " 9   length_type         86830 non-null  object \n",
      " 10  silhouette_type     82972 non-null  object \n",
      " 11  waist_type          23252 non-null  object \n",
      " 12  neck_lapel_type     58874 non-null  object \n",
      " 13  sleeve_length_type  57336 non-null  object \n",
      " 14  heel_shape_type     0 non-null      float64\n",
      " 15  toecap_type         0 non-null      float64\n",
      " 16  woven_structure     62510 non-null  object \n",
      " 17  knit_structure      16926 non-null  object \n",
      " 18  print_type          95239 non-null  object \n",
      " 19  archetype           55607 non-null  object \n",
      " 20  moment              95339 non-null  object \n",
      " 21  phase_in            95339 non-null  object \n",
      " 22  phase_out           95339 non-null  object \n",
      " 23  life_cycle_length   95339 non-null  int64  \n",
      " 24  num_stores          95339 non-null  int64  \n",
      " 25  num_sizes           95339 non-null  int64  \n",
      " 26  has_plus_sizes      95339 non-null  bool   \n",
      " 27  price               95339 non-null  float64\n",
      " 28  year                95339 non-null  int64  \n",
      " 29  num_week_iso        95339 non-null  int64  \n",
      " 30  weekly_sales        95339 non-null  int64  \n",
      " 31  weekly_demand       95339 non-null  int64  \n",
      " 32  Production          95339 non-null  int64  \n",
      "dtypes: bool(1), float64(3), int64(10), object(19)\n",
      "memory usage: 23.4+ MB\n",
      "\n",
      "\n",
      "--- Información de test_df (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2250 entries, 0 to 2249\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  2250 non-null   int64  \n",
      " 1   id_season           2250 non-null   int64  \n",
      " 2   aggregated_family   2250 non-null   object \n",
      " 3   family              2250 non-null   object \n",
      " 4   category            2250 non-null   object \n",
      " 5   fabric              2250 non-null   object \n",
      " 6   color_name          2250 non-null   object \n",
      " 7   color_rgb           2250 non-null   object \n",
      " 8   image_embedding     2250 non-null   object \n",
      " 9   length_type         1868 non-null   object \n",
      " 10  silhouette_type     1771 non-null   object \n",
      " 11  waist_type          450 non-null    object \n",
      " 12  neck_lapel_type     1387 non-null   object \n",
      " 13  sleeve_length_type  1336 non-null   object \n",
      " 14  heel_shape_type     0 non-null      float64\n",
      " 15  toecap_type         0 non-null      float64\n",
      " 16  woven_structure     1382 non-null   object \n",
      " 17  knit_structure      395 non-null    object \n",
      " 18  print_type          2247 non-null   object \n",
      " 19  archetype           200 non-null    object \n",
      " 20  moment              2250 non-null   object \n",
      " 21  phase_in            2250 non-null   object \n",
      " 22  phase_out           2250 non-null   object \n",
      " 23  life_cycle_length   2250 non-null   int64  \n",
      " 24  num_stores          2250 non-null   int64  \n",
      " 25  num_sizes           2250 non-null   int64  \n",
      " 26  has_plus_sizes      2250 non-null   bool   \n",
      " 27  price               2250 non-null   float64\n",
      " 28  Unnamed: 28         0 non-null      float64\n",
      " 29  Unnamed: 29         0 non-null      float64\n",
      " 30  Unnamed: 30         0 non-null      float64\n",
      " 31  Unnamed: 31         0 non-null      float64\n",
      " 32  Unnamed: 32         0 non-null      float64\n",
      "dtypes: bool(1), float64(8), int64(5), object(19)\n",
      "memory usage: 564.8+ KB\n",
      "\n",
      "\n",
      "--- Conteo de filas por 'id' en train_df ---\n",
      "ID\n",
      "983      31\n",
      "2520     31\n",
      "11792    31\n",
      "7845     31\n",
      "6056     31\n",
      "12543    30\n",
      "5100     30\n",
      "3596     30\n",
      "3476     30\n",
      "6439     30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- Conteo de filas por 'id' en test_df ---\n",
      "ID\n",
      "90     1\n",
      "16     1\n",
      "65     1\n",
      "138    1\n",
      "166    1\n",
      "252    1\n",
      "234    1\n",
      "306    1\n",
      "274    1\n",
      "268    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 1: Tarea 2 (Inspección Rápida) ---\n",
    "\n",
    "print(\"--- Vistazo a train_df (head) ---\")\n",
    "print(train_df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Información de train_df (.info()) ---\")\n",
    "# Esto nos dirá los tipos de datos (Dtypes) y los nulos\n",
    "train_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Información de test_df (.info()) ---\")\n",
    "test_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Paso 1: Tarea 3 (Validar Hipótesis de Agregación) ---\n",
    "\n",
    "print(\"--- Conteo de filas por 'id' en train_df ---\")\n",
    "# Si vemos números mayores a 1, significa que hay varias filas por id (semanal)\n",
    "print(train_df['ID'].value_counts().head(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Conteo de filas por 'id' en test_df ---\")\n",
    "# Aquí deberíamos ver que todos los valores son '1'\n",
    "print(test_df['ID'].value_counts().head(10))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6b6eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregando 'weekly_demand' para crear el target (y_train_agg)...\n",
      "Target (y_train_agg) creado. Ejemplo:\n",
      "ID\n",
      "1      806\n",
      "2     2266\n",
      "3    63791\n",
      "4    11004\n",
      "6    14684\n",
      "Name: weekly_demand, dtype: int64\n",
      "\n",
      "\n",
      "Agregando features (X_train_agg)...\n",
      "Encontradas 27 columnas en común.\n",
      "\n",
      "¡Agregación completada y alineada con éxito!\n",
      "Forma de X_train_agg: (9843, 27)\n",
      "Forma de y_train_agg: (9843,)\n",
      "Forma de test_df_clean: (2250, 27)\n",
      "\n",
      "\n",
      "--- Columnas de X_train_agg (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9843 entries, 1 to 12767\n",
      "Data columns (total 27 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   moment              9843 non-null   object \n",
      " 1   archetype           5663 non-null   object \n",
      " 2   phase_in            9843 non-null   object \n",
      " 3   neck_lapel_type     6407 non-null   object \n",
      " 4   phase_out           9843 non-null   object \n",
      " 5   color_rgb           9843 non-null   object \n",
      " 6   print_type          9838 non-null   object \n",
      " 7   num_stores          9843 non-null   int64  \n",
      " 8   heel_shape_type     0 non-null      float64\n",
      " 9   has_plus_sizes      9843 non-null   bool   \n",
      " 10  knit_structure      1965 non-null   object \n",
      " 11  waist_type          2140 non-null   object \n",
      " 12  price               9843 non-null   float64\n",
      " 13  silhouette_type     8638 non-null   object \n",
      " 14  toecap_type         0 non-null      float64\n",
      " 15  family              9843 non-null   object \n",
      " 16  life_cycle_length   9843 non-null   int64  \n",
      " 17  length_type         9046 non-null   object \n",
      " 18  image_embedding     9843 non-null   object \n",
      " 19  color_name          9843 non-null   object \n",
      " 20  category            9843 non-null   object \n",
      " 21  woven_structure     6355 non-null   object \n",
      " 22  id_season           9843 non-null   int64  \n",
      " 23  aggregated_family   9843 non-null   object \n",
      " 24  sleeve_length_type  6271 non-null   object \n",
      " 25  fabric              9843 non-null   object \n",
      " 26  num_sizes           9843 non-null   int64  \n",
      "dtypes: bool(1), float64(3), int64(4), object(19)\n",
      "memory usage: 2.0+ MB\n",
      "\n",
      "\n",
      "--- Columnas de test_df_clean (.info()) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2250 entries, 0 to 2249\n",
      "Data columns (total 27 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   moment              2250 non-null   object \n",
      " 1   archetype           200 non-null    object \n",
      " 2   phase_in            2250 non-null   object \n",
      " 3   neck_lapel_type     1387 non-null   object \n",
      " 4   phase_out           2250 non-null   object \n",
      " 5   color_rgb           2250 non-null   object \n",
      " 6   print_type          2247 non-null   object \n",
      " 7   num_stores          2250 non-null   int64  \n",
      " 8   heel_shape_type     0 non-null      float64\n",
      " 9   has_plus_sizes      2250 non-null   bool   \n",
      " 10  knit_structure      395 non-null    object \n",
      " 11  waist_type          450 non-null    object \n",
      " 12  price               2250 non-null   float64\n",
      " 13  silhouette_type     1771 non-null   object \n",
      " 14  toecap_type         0 non-null      float64\n",
      " 15  family              2250 non-null   object \n",
      " 16  life_cycle_length   2250 non-null   int64  \n",
      " 17  length_type         1868 non-null   object \n",
      " 18  image_embedding     2250 non-null   object \n",
      " 19  color_name          2250 non-null   object \n",
      " 20  category            2250 non-null   object \n",
      " 21  woven_structure     1382 non-null   object \n",
      " 22  id_season           2250 non-null   int64  \n",
      " 23  aggregated_family   2250 non-null   object \n",
      " 24  sleeve_length_type  1336 non-null   object \n",
      " 25  fabric              2250 non-null   object \n",
      " 26  num_sizes           2250 non-null   int64  \n",
      "dtypes: bool(1), float64(3), int64(4), object(19)\n",
      "memory usage: 459.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- [INICIO] Código Corregido para Paso 2 ---\n",
    "\n",
    "# --- Paso 2: Tarea 1 (Crear el Target 'y_train_agg') ---\n",
    "print(\"Agregando 'weekly_demand' para crear el target (y_train_agg)...\")\n",
    "\n",
    "# (CORRECCIÓN): Usamos 'ID' (mayúscula) como vimos en tu CSV\n",
    "y_train_agg = train_df.groupby('ID')['weekly_demand'].sum()\n",
    "\n",
    "print(\"Target (y_train_agg) creado. Ejemplo:\")\n",
    "print(y_train_agg.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Paso 2: Tarea 2 (Crear las Features 'X_train_agg') ---\n",
    "print(\"Agregando features (X_train_agg)...\")\n",
    "\n",
    "# (CORRECCIÓN): Usamos 'ID' (mayúscula)\n",
    "X_train_agg_full = train_df.groupby('ID').first()\n",
    "\n",
    "\n",
    "# --- Tarea 2b: Alinear Columnas (¡NUEVA LÓGICA MÁS ROBUSTA!) ---\n",
    "# En lugar de usar la lista 'test_df.columns' (que tiene 'ID' y 'Unnamed'),\n",
    "# vamos a buscar las columnas que SÍ están en ambos sitios.\n",
    "\n",
    "train_features = set(X_train_agg_full.columns)\n",
    "test_features = set(test_df.columns)\n",
    "\n",
    "# Buscamos la intersección: columnas que están en X_train_agg_full Y en test_df\n",
    "# Esto excluye automáticamente 'ID' (que no es columna en train) \n",
    "# y 'Unnamed:' (que no están en train)\n",
    "common_columns = list(train_features.intersection(test_features))\n",
    "\n",
    "print(f\"Encontradas {len(common_columns)} columnas en común.\")\n",
    "\n",
    "# Ahora filtramos ambos DataFrames para que SÓLO tengan estas columnas\n",
    "X_train_agg = X_train_agg_full[common_columns].copy() # Usamos .copy() para evitar warnings\n",
    "\n",
    "# ¡Importante! También limpiamos test_df para que coincida\n",
    "# (Guardamos el 'ID' original de test_df para la sumisión final)\n",
    "test_ids_for_submission = test_df['ID']\n",
    "test_df_clean = test_df[common_columns].copy() \n",
    "\n",
    "\n",
    "# --- Tarea 2c: Alineación Final ---\n",
    "# Nos aseguramos de que 'y_train_agg' siga el mismo orden que 'X_train_agg'\n",
    "y_train_agg = y_train_agg.reindex(X_train_agg.index)\n",
    "\n",
    "print(\"\\n¡Agregación completada y alineada con éxito!\")\n",
    "print(f\"Forma de X_train_agg: {X_train_agg.shape}\")\n",
    "print(f\"Forma de y_train_agg: {y_train_agg.shape}\")\n",
    "print(f\"Forma de test_df_clean: {test_df_clean.shape}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Columnas de X_train_agg (.info()) ---\")\n",
    "X_train_agg.info()\n",
    "print(\"\\n\")\n",
    "print(\"--- Columnas de test_df_clean (.info()) ---\")\n",
    "test_df_clean.info()\n",
    "\n",
    "# --- [FIN] Código Corregido para Paso 2 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "608a26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 3: Ingeniería de Características...\n",
      "Feature 'start_month' creada con éxito. Ejemplo:\n",
      "     phase_in  start_month\n",
      "ID                        \n",
      "1  2023-02-01          2.0\n",
      "2         NaT          6.0\n",
      "3         NaT          6.0\n",
      "4         NaT          6.0\n",
      "6         NaT          6.0\n",
      "\n",
      "\n",
      "Definidas 5 features numéricas:\n",
      "['num_stores', 'price', 'life_cycle_length', 'num_sizes', 'start_month']\n",
      "\n",
      "\n",
      "Definidas 17 features categóricas:\n",
      "['moment', 'archetype', 'neck_lapel_type', 'print_type', 'has_plus_sizes', 'knit_structure', 'waist_type', 'silhouette_type', 'family', 'length_type', 'color_name', 'category', 'woven_structure', 'id_season', 'aggregated_family', 'sleeve_length_type', 'fabric']\n",
      "\n",
      "¡Paso 3 completado! Estamos listos para construir el Pipeline.\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 3: Ingeniería de Características ---\n",
    "\n",
    "print(\"Iniciando Paso 3: Ingeniería de Características...\")\n",
    "\n",
    "# Hacemos una copia para evitar 'SettingWithCopyWarning'\n",
    "X_train_features = X_train_agg.copy()\n",
    "X_test_features = test_df_clean.copy()\n",
    "\n",
    "# --- Tarea 1: Crear 'start_month' (Ingeniería de Fechas) ---\n",
    "# Convertimos 'phase_in' a formato fecha (MANDATORIO para extraer el mes)\n",
    "# errors='coerce' convertirá cualquier fecha inválida en NaT (Nulo)\n",
    "X_train_features['phase_in'] = pd.to_datetime(X_train_features['phase_in'], errors='coerce')\n",
    "X_test_features['phase_in'] = pd.to_datetime(X_test_features['phase_in'], errors='coerce')\n",
    "\n",
    "# Extraemos el mes (1-12) como una nueva feature numérica\n",
    "X_train_features['start_month'] = X_train_features['phase_in'].dt.month\n",
    "X_test_features['start_month'] = X_test_features['phase_in'].dt.month\n",
    "\n",
    "# Ahora rellenamos cualquier nulo que se haya creado (por si 'errors=coerce' falló)\n",
    "# Usaremos la mediana (ej. 6, para Junio/Julio)\n",
    "median_month = X_train_features['start_month'].median()\n",
    "X_train_features['start_month'] = X_train_features['start_month'].fillna(median_month)\n",
    "X_test_features['start_month'] = X_test_features['start_month'].fillna(median_month)\n",
    "\n",
    "print(\"Feature 'start_month' creada con éxito. Ejemplo:\")\n",
    "print(X_train_features[['phase_in', 'start_month']].head())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Tarea 2: Definir Listas de Columnas para el Pipeline ---\n",
    "# Basado en nuestro análisis del .info()\n",
    "\n",
    "# 1. Columnas Numéricas: Rellenaremos nulos con la mediana y las escalaremos\n",
    "numeric_features = [\n",
    "    'num_stores',\n",
    "    'price',\n",
    "    'life_cycle_length',\n",
    "    'num_sizes',\n",
    "    'start_month'  # Nuestra nueva feature\n",
    "]\n",
    "\n",
    "# 2. Columnas Categóricas: Rellenaremos nulos con \"Desconocido\" y aplicaremos One-Hot Encoding\n",
    "categorical_features = [\n",
    "    'moment',\n",
    "    'archetype',\n",
    "    'neck_lapel_type',\n",
    "    'print_type',\n",
    "    'has_plus_sizes',       # Aunque es bool, lo tratamos como categórico\n",
    "    'knit_structure',\n",
    "    'waist_type',\n",
    "    'silhouette_type',\n",
    "    'family',\n",
    "    'length_type',\n",
    "    'color_name',\n",
    "    'category',\n",
    "    'woven_structure',\n",
    "    'id_season',            # Aunque es int, es una CATEGORÍA (ej. temporada 86)\n",
    "    'aggregated_family',\n",
    "    'sleeve_length_type',\n",
    "    'fabric'\n",
    "]\n",
    "\n",
    "# 3. Columnas a Eliminar: No las usaremos\n",
    "# (El Pipeline se encargará de esto automáticamente al NO incluirlas \n",
    "# en las listas de arriba, excepto las que creamos y ya no necesitamos)\n",
    "\n",
    "print(f\"Definidas {len(numeric_features)} features numéricas:\")\n",
    "print(numeric_features)\n",
    "print(\"\\n\")\n",
    "print(f\"Definidas {len(categorical_features)} features categóricas:\")\n",
    "print(categorical_features)\n",
    "print(\"\\n¡Paso 3 completado! Estamos listos para construir el Pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f01e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas de Scikit-learn importadas.\n",
      "Pipelines de transformación numérica y categórica definidos.\n",
      "ColumnTransformer ('preprocessor') creado con éxito.\n",
      "\n",
      "¡Paso 4 completado! El 'preprocessor' está listo para ser usado.\n",
      "\n",
      "--- Verificación del Preprocesador ---\n",
      "Forma original de X_train: (9843, 28)\n",
      "Forma de X_train procesado: (9843, 297)\n",
      "¡El número de columnas ha crecido por el OneHotEncoding!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 4: Preprocesamiento y Pipeline de Scikit-learn ---\n",
    "\n",
    "# Importamos todas las herramientas que necesitamos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "print(\"Herramientas de Scikit-learn importadas.\")\n",
    "\n",
    "# --- Tarea 1: Definir los Mini-Pipelines ---\n",
    "\n",
    "# Pipeline para datos NUMÉRICOS\n",
    "# 1. SimpleImputer: Rellena cualquier nulo (ej. en 'start_month') con la mediana.\n",
    "# 2. StandardScaler: Escala los datos (pone todo ~ entre -2 y 2). \n",
    "#    RandomForest no lo necesita, pero es BUENA PRÁCTICA para otros modelos.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para datos CATEGÓRICOS\n",
    "# 1. SimpleImputer: Rellena los nulos (ej. en 'archetype') con la palabra \"Desconocido\".\n",
    "# 2. OneHotEncoder: Crea nuevas columnas para cada categoría (ej. family_Dresses, family_Coats).\n",
    "#    handle_unknown='ignore' -> Si en test_df aparece un color que no vio en train,\n",
    "#    simplemente lo ignora sin dar error. ¡Vital!\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "print(\"Pipelines de transformación numérica y categórica definidos.\")\n",
    "\n",
    "# --- Tarea 2: Unir con ColumnTransformer ---\n",
    "# Aquí es donde le decimos a sklearn:\n",
    "# - Aplica 'numeric_transformer' a las columnas de 'numeric_features'\n",
    "# - Aplica 'categorical_transformer' a las columnas de 'categorical_features'\n",
    "# - remainder='drop' -> Todas las columnas que no mencionamos (ej. image_embedding) serán ELIMINADAS.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Ignora las columnas que no están en nuestras listas\n",
    ")\n",
    "\n",
    "print(\"ColumnTransformer ('preprocessor') creado con éxito.\")\n",
    "print(\"\\n¡Paso 4 completado! El 'preprocessor' está listo para ser usado.\")\n",
    "\n",
    "# --- Bonus: Verifiquemos qué ha creado ---\n",
    "# Vamos a \"entrenar\" solo el preprocesador y ver la forma de la salida\n",
    "# Usamos las variables que creamos en el Paso 3\n",
    "\n",
    "# Renombramos por simplicidad\n",
    "X = X_train_features\n",
    "y = y_train_agg\n",
    "X_test = X_test_features\n",
    "\n",
    "# 'fit_transform' aprende de X (ej. calcula la mediana) y luego transforma X\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- Verificación del Preprocesador ---\")\n",
    "print(f\"Forma original de X_train: {X.shape}\")\n",
    "print(f\"Forma de X_train procesado: {X_processed.shape}\")\n",
    "print(\"¡El número de columnas ha crecido por el OneHotEncoding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a4a21a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas de modelo y métricas importadas.\n",
      "Datos divididos: 7874 para entrenar, 1969 para validar.\n",
      "\n",
      "\n",
      "Pipeline final creado (Preprocesador + RandomForest).\n",
      "Entrenando el modelo... (Esto puede tardar 1-2 minutos)\n",
      "¡Modelo entrenado con éxito!\n",
      "\n",
      "\n",
      "--- Evaluación del Modelo (sobre datos de validación) ---\n",
      "Error Absoluto Medio (MAE): 3822.0330\n",
      "Raíz del Error Cuadrático Medio (RMSE): 7800.2950\n",
      "\n",
      "\n",
      "--- Interpretación ---\n",
      "Un MAE de 3822.0330 significa que, en promedio, las predicciones de demanda total del modelo\n",
      "tienen un error de +/- 3822.0330 unidades (en la escala 0-1).\n",
      "\n",
      "¡Paso 5 y 6 completados!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 5: Entrenamiento del Modelo ---\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"Herramientas de modelo y métricas importadas.\")\n",
    "\n",
    "# --- Tarea 1: Separar Datos de Validación ---\n",
    "# Dividimos nuestros datos (X, y) en 80% para entrenar y 20% para validar\n",
    "# random_state=42 asegura que la división sea siempre la misma\n",
    "X_train_local, X_val_local, y_train_local, y_val_local = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Datos divididos: {len(X_train_local)} para entrenar, {len(X_val_local)} para validar.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Tarea 2: Crear el Pipeline Final ---\n",
    "# Unimos el 'preprocessor' (Paso 4) y nuestro modelo 'RandomForestRegressor'\n",
    "\n",
    "# n_estimators=100 -> Crear un bosque de 100 árboles (es un buen número para empezar)\n",
    "# n_jobs=-1 -> Usar todos los núcleos de tu CPU para entrenar más rápido\n",
    "# random_state=42 -> Para que el entrenamiento sea reproducible\n",
    "model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "print(\"Pipeline final creado (Preprocesador + RandomForest).\")\n",
    "\n",
    "# --- Tarea 3: Entrenar ---\n",
    "print(\"Entrenando el modelo... (Esto puede tardar 1-2 minutos)\")\n",
    "# ¡Aquí ocurre la magia!\n",
    "# .fit() entrena el preprocesador y el modelo, todo en una línea.\n",
    "full_pipeline.fit(X_train_local, y_train_local)\n",
    "\n",
    "print(\"¡Modelo entrenado con éxito!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Paso 6: Evaluación y Ajuste ---\n",
    "\n",
    "print(\"--- Evaluación del Modelo (sobre datos de validación) ---\")\n",
    "\n",
    "# 1. Predecir sobre los datos de validación\n",
    "y_pred_val = full_pipeline.predict(X_val_local)\n",
    "\n",
    "# 2. Calcular Métricas de Error\n",
    "mae = mean_absolute_error(y_val_local, y_pred_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_local, y_pred_val))\n",
    "\n",
    "print(f\"Error Absoluto Medio (MAE): {mae:.4f}\")\n",
    "print(f\"Raíz del Error Cuadrático Medio (RMSE): {rmse:.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Interpretación ---\")\n",
    "print(f\"Un MAE de {mae:.4f} significa que, en promedio, las predicciones de demanda total del modelo\")\n",
    "print(f\"tienen un error de +/- {mae:.4f} unidades (en la escala 0-1).\")\n",
    "print(\"\\n¡Paso 5 y 6 completados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7715f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Paso 7: Predicción Final...\n",
      "Re-entrenando el modelo con el 100% de los datos de entrenamiento...\n",
      "¡Modelo final entrenado!\n",
      "Generando predicciones sobre los datos de test...\n",
      "¡Predicciones generadas!\n",
      "Predicciones negativas ajustadas a 0.\n",
      "\n",
      "\n",
      "==================================================\n",
      "¡Archivo 'submission2.csv' creado con éxito!\n",
      "==================================================\n",
      "Vistazo al archivo de envío:\n",
      "    ID   demand\n",
      "0   90  1272.69\n",
      "1   16  6059.90\n",
      "2   65  9059.40\n",
      "3  138   820.80\n",
      "4  166   328.27\n",
      "\n",
      "\n",
      "¡Paso 7 completado!\n"
     ]
    }
   ],
   "source": [
    "# --- Paso 7: Predicción Final y Archivo de Envío ---\n",
    "\n",
    "print(\"Iniciando Paso 7: Predicción Final...\")\n",
    "\n",
    "# --- Tarea 1: Entrenar con TODOS los datos ---\n",
    "print(\"Re-entrenando el modelo con el 100% de los datos de entrenamiento...\")\n",
    "# Usamos 'X' e 'y' (los DataFrames completos que creamos en el Paso 3)\n",
    "# 'X_test_features' es el set de test limpio\n",
    "full_pipeline.fit(X, y)\n",
    "\n",
    "print(\"¡Modelo final entrenado!\")\n",
    "\n",
    "# --- Tarea 2: Predecir sobre test_df_clean ---\n",
    "print(\"Generando predicciones sobre los datos de test...\")\n",
    "# (Recuerda que X_test_features es el nombre que le dimos a test_df_clean en el Paso 3)\n",
    "final_predictions = full_pipeline.predict(X_test_features)\n",
    "\n",
    "print(\"¡Predicciones generadas!\")\n",
    "\n",
    "# --- Tarea 3: Post-Procesamiento (¡MUY IMPORTANTE!) ---\n",
    "# La demanda no puede ser negativa.\n",
    "# Reemplazamos cualquier predicción negativa por 0.\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "print(\"Predicciones negativas ajustadas a 0.\")\n",
    "\n",
    "# --- Tarea 4: Crear Archivo de Envío ---\n",
    "# Usamos los 'test_ids_for_submission' que guardamos en el Paso 2\n",
    "# y nuestras 'final_predictions'\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids_for_submission,\n",
    "    'demand': final_predictions\n",
    "})\n",
    "\n",
    "# --- Tarea 5: Guardar el Archivo ---\n",
    "submission_filename = 'submission2.csv'\n",
    "submission_df.to_csv(submission_filename, index=False, sep=',')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"¡Archivo '{submission_filename}' creado con éxito!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Vistazo al archivo de envío:\")\n",
    "print(submission_df.head())\n",
    "print(\"\\n\")\n",
    "print(\"¡Paso 7 completado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
