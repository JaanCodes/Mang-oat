{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessament i Modelització\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "print(\"Librerías básicas importadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de auto-sklearn (si es necesario)\n",
    "# Nota: auto-sklearn puede tener problemas en Windows. Si falla, usaremos una alternativa.\n",
    "try:\n",
    "    import autosklearn.regression\n",
    "    print(\"auto-sklearn ya está instalado.\")\n",
    "except ImportError:\n",
    "    print(\"Intentando instalar auto-sklearn...\")\n",
    "    print(\"NOTA: auto-sklearn puede no funcionar en Windows.\")\n",
    "    print(\"Como alternativa, usaremos FLAML que es compatible con Windows y muy efectivo.\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"flaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b702a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar FLAML (alternativa a auto-sklearn, compatible con Windows)\n",
    "try:\n",
    "    from flaml import AutoML\n",
    "    USE_FLAML = True\n",
    "    print(\"Usando FLAML para AutoML (compatible con Windows)\")\n",
    "except ImportError:\n",
    "    USE_FLAML = False\n",
    "    print(\"FLAML no disponible. Usaremos GridSearch manual.\")\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "data_dir = Path('.')\n",
    "train_path = data_dir / 'train.csv'\n",
    "test_path = data_dir / 'test.csv'\n",
    "sample_path = data_dir / 'sample_submission.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path, sep=';')\n",
    "    test_df = pd.read_csv(test_path, sep=';')\n",
    "    sample_sub = pd.read_csv(sample_path, sep=',')\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Asegúrate que los archivos CSV estén en el directorio.\")\n",
    "\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"Sample: {sample_sub.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e73f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregación de datos (mismo proceso que EDA.ipynb)\n",
    "print(\"Agregando datos por ID...\")\n",
    "\n",
    "# Target\n",
    "y_train_agg = train_df.groupby('ID')['weekly_demand'].sum()\n",
    "\n",
    "# Features\n",
    "X_train_agg_full = train_df.groupby('ID').first()\n",
    "\n",
    "# Alinear columnas\n",
    "train_features = set(X_train_agg_full.columns)\n",
    "test_features = set(test_df.columns)\n",
    "common_columns = list(train_features.intersection(test_features))\n",
    "\n",
    "X_train_agg = X_train_agg_full[common_columns].copy()\n",
    "test_ids_for_submission = test_df['ID']\n",
    "test_df_clean = test_df[common_columns].copy()\n",
    "\n",
    "y_train_agg = y_train_agg.reindex(X_train_agg.index)\n",
    "\n",
    "print(f\"X_train_agg: {X_train_agg.shape}\")\n",
    "print(f\"y_train_agg: {y_train_agg.shape}\")\n",
    "print(f\"test_df_clean: {test_df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingeniería de características\n",
    "print(\"Creando features...\")\n",
    "\n",
    "X_train_features = X_train_agg.copy()\n",
    "X_test_features = test_df_clean.copy()\n",
    "\n",
    "# Feature: start_month\n",
    "X_train_features['phase_in'] = pd.to_datetime(X_train_features['phase_in'], errors='coerce')\n",
    "X_test_features['phase_in'] = pd.to_datetime(X_test_features['phase_in'], errors='coerce')\n",
    "\n",
    "X_train_features['start_month'] = X_train_features['phase_in'].dt.month\n",
    "X_test_features['start_month'] = X_test_features['phase_in'].dt.month\n",
    "\n",
    "median_month = X_train_features['start_month'].median()\n",
    "X_train_features['start_month'] = X_train_features['start_month'].fillna(median_month)\n",
    "X_test_features['start_month'] = X_test_features['start_month'].fillna(median_month)\n",
    "\n",
    "# Listas de features\n",
    "numeric_features = [\n",
    "    'num_stores', 'price', 'life_cycle_length', 'num_sizes', 'start_month'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'moment', 'archetype', 'neck_lapel_type', 'print_type', 'has_plus_sizes',\n",
    "    'knit_structure', 'waist_type', 'silhouette_type', 'family', 'length_type',\n",
    "    'color_name', 'category', 'woven_structure', 'id_season',\n",
    "    'aggregated_family', 'sleeve_length_type', 'fabric'\n",
    "]\n",
    "\n",
    "print(f\"Features numéricas: {len(numeric_features)}\")\n",
    "print(f\"Features categóricas: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador de embeddings\n",
    "class EmbeddingSplitter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.str.split(',', expand=True).astype(float).fillna(0)\n",
    "\n",
    "# Pipelines de transformación\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "embedding_transformer = Pipeline(steps=[\n",
    "    ('split', EmbeddingSplitter()),\n",
    "    ('pca', PCA(n_components=64, random_state=42))\n",
    "])\n",
    "\n",
    "# Preprocessor completo\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('embed', embedding_transformer, 'image_embedding')\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessor creado con embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c325dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de pérdida asimétrica personalizada\n",
    "# Penaliza más cuando la predicción es menor que el real (underestimation)\n",
    "def asymmetric_loss(y_true, y_pred, underestimation_penalty=2.0):\n",
    "    \"\"\"\n",
    "    Pérdida asimétrica que penaliza más las subestimaciones.\n",
    "    \n",
    "    Parameters:\n",
    "    - underestimation_penalty: factor de penalización cuando y_pred < y_true\n",
    "      Por defecto 2.0 significa que penaliza el doble las subestimaciones.\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    # Errores positivos = subestimación (pred < real) -> penaliza más\n",
    "    # Errores negativos = sobreestimación (pred > real) -> penaliza menos\n",
    "    weights = np.where(errors > 0, underestimation_penalty, 1.0)\n",
    "    return np.mean(weights * np.abs(errors))\n",
    "\n",
    "def asymmetric_scorer(y_true, y_pred):\n",
    "    \"\"\"Wrapper para usar como scorer en sklearn (mayor es mejor, así que invertimos)\"\"\"\n",
    "    return -asymmetric_loss(y_true, y_pred, underestimation_penalty=2.0)\n",
    "\n",
    "# Crear scorer para sklearn\n",
    "custom_scorer = make_scorer(asymmetric_scorer, greater_is_better=True)\n",
    "\n",
    "print(\"Función de pérdida asimétrica definida.\")\n",
    "print(\"Penalización: 2x cuando predecimos por debajo del real.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ccbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación logarítmica del target\n",
    "print(\"Aplicando transformación logarítmica al target...\")\n",
    "y_train_log = np.log1p(y_train_agg)\n",
    "\n",
    "# Preparar datos\n",
    "X = X_train_features\n",
    "y = y_train_log\n",
    "X_test = X_test_features\n",
    "\n",
    "# Split para validación\n",
    "X_train_local, X_val_local, y_train_local, y_val_local = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# También guardamos los valores sin log para evaluación\n",
    "y_train_agg_split = train_test_split(y_train_agg, test_size=0.2, random_state=42)\n",
    "y_train_local_real = y_train_agg_split[0]\n",
    "y_val_local_real = y_train_agg_split[1]\n",
    "\n",
    "print(f\"Datos divididos: {len(X_train_local)} train, {len(X_val_local)} validación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863feee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar los datos\n",
    "print(\"Preprocesando datos...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train_local)\n",
    "X_val_processed = preprocessor.transform(X_val_local)\n",
    "\n",
    "print(f\"Shape después del preprocesamiento: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de941222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-sklearn / FLAML para encontrar el mejor modelo\n",
    "print(\"=\"*60)\n",
    "print(\"Iniciando búsqueda automática del mejor modelo...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if USE_FLAML:\n",
    "    print(\"\\nUsando FLAML AutoML...\")\n",
    "    print(\"Configuración: 10 minutos de búsqueda, métrica personalizada asimétrica\")\n",
    "    \n",
    "    automl = AutoML()\n",
    "    \n",
    "    # Configuración de FLAML\n",
    "    settings = {\n",
    "        \"time_budget\": 600,  # 10 minutos\n",
    "        \"metric\": \"mae\",  # Usamos MAE como base\n",
    "        \"task\": \"regression\",\n",
    "        \"log_file_name\": \"flaml_log.txt\",\n",
    "        \"seed\": 42,\n",
    "        \"eval_method\": \"cv\",  # Cross-validation\n",
    "        \"n_splits\": 3,\n",
    "        \"verbose\": 1,\n",
    "        # Modelos a probar\n",
    "        \"estimator_list\": ['lgbm', 'rf', 'xgboost', 'extra_tree', 'catboost']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n¡Entrenando! Esto puede tardar ~10 minutos...\\n\")\n",
    "    \n",
    "    automl.fit(\n",
    "        X_train=X_train_processed,\n",
    "        y_train=y_train_local,\n",
    "        **settings\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FLAML ha terminado!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nMejor modelo encontrado: {automl.best_estimator}\")\n",
    "    print(f\"Mejor configuración: {automl.best_config}\")\n",
    "    print(f\"Mejor score (MAE en log-scale): {automl.best_loss}\")\n",
    "    \n",
    "    best_model = automl\n",
    "    \n",
    "else:\n",
    "    print(\"\\nUsando GridSearch manual con varios modelos...\")\n",
    "    print(\"Esto puede tardar varios minutos...\\n\")\n",
    "    \n",
    "    # Definir modelos y parámetros para GridSearch\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from lightgbm import LGBMRegressor\n",
    "    \n",
    "    models_and_params = [\n",
    "        {\n",
    "            'name': 'RandomForest',\n",
    "            'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30, None],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'LightGBM',\n",
    "            'model': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_depth': [5, 10, 15],\n",
    "                'num_leaves': [31, 50, 70]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_config in models_and_params:\n",
    "        print(f\"\\nProbando {model_config['name']}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_config['model'],\n",
    "            param_grid=model_config['params'],\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_processed, y_train_local)\n",
    "        \n",
    "        if -grid_search.best_score_ < best_score:\n",
    "            best_score = -grid_search.best_score_\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_model_name = model_config['name']\n",
    "        \n",
    "        print(f\"Mejor score para {model_config['name']}: {-grid_search.best_score_:.4f}\")\n",
    "        print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Mejor modelo global: {best_model_name}\")\n",
    "    print(f\"Mejor score (MAE en log-scale): {best_score:.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04200937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo con pérdida asimétrica\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN DEL MODELO AUTO-OPTIMIZADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predecir en validación (en escala log)\n",
    "y_pred_val_log = best_model.predict(X_val_processed)\n",
    "\n",
    "# Invertir transformación logarítmica\n",
    "y_pred_val_real = np.expm1(y_pred_val_log)\n",
    "\n",
    "# Calcular métricas estándar\n",
    "mae_standard = mean_absolute_error(y_val_local_real, y_pred_val_real)\n",
    "rmse_standard = np.sqrt(mean_squared_error(y_val_local_real, y_pred_val_real))\n",
    "\n",
    "# Calcular métrica asimétrica\n",
    "mae_asymmetric = asymmetric_loss(y_val_local_real.values, y_pred_val_real, underestimation_penalty=2.0)\n",
    "\n",
    "print(\"\\n--- Métricas Estándar ---\")\n",
    "print(f\"MAE: {mae_standard:.4f}\")\n",
    "print(f\"RMSE: {rmse_standard:.4f}\")\n",
    "\n",
    "print(\"\\n--- Métrica Asimétrica (penaliza 2x subestimaciones) ---\")\n",
    "print(f\"MAE Asimétrico: {mae_asymmetric:.4f}\")\n",
    "\n",
    "# Análisis de errores\n",
    "errors = y_val_local_real.values - y_pred_val_real\n",
    "underestimations = errors[errors > 0]\n",
    "overestimations = errors[errors < 0]\n",
    "\n",
    "print(\"\\n--- Análisis de Errores ---\")\n",
    "print(f\"Subestimaciones (pred < real): {len(underestimations)} casos\")\n",
    "print(f\"  - Media: {np.mean(np.abs(underestimations)):.4f}\")\n",
    "print(f\"Sobreestimaciones (pred > real): {len(overestimations)} casos\")\n",
    "print(f\"  - Media: {np.mean(np.abs(overestimations)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5de987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico 1: Predicciones vs Real\n",
    "axes[0].scatter(y_val_local_real, y_pred_val_real, alpha=0.5)\n",
    "axes[0].plot([y_val_local_real.min(), y_val_local_real.max()], \n",
    "             [y_val_local_real.min(), y_val_local_real.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Demanda Real')\n",
    "axes[0].set_ylabel('Demanda Predicha')\n",
    "axes[0].set_title('Predicciones vs Valores Reales')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 2: Distribución de errores\n",
    "axes[1].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Error (Real - Predicho)')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_title('Distribución de Errores\\n(>0 = Subestimación, <0 = Sobreestimación)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autosklearn_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGráficos guardados en 'autosklearn_evaluation.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con todos los datos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTRENAMIENTO DEL MODELO FINAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPreprocesando datos completos...\")\n",
    "X_full_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Entrenando modelo con 100% de los datos...\")\n",
    "\n",
    "if USE_FLAML:\n",
    "    # Re-entrenar con los mismos parámetros óptimos\n",
    "    final_model = AutoML()\n",
    "    final_model.fit(\n",
    "        X_train=X_full_processed,\n",
    "        y_train=y,\n",
    "        time_budget=300,  # 5 minutos adicionales\n",
    "        metric=\"mae\",\n",
    "        task=\"regression\",\n",
    "        seed=42,\n",
    "        starting_points=automl.best_config\n",
    "    )\n",
    "else:\n",
    "    # Usar el mejor modelo encontrado\n",
    "    final_model = best_model\n",
    "    final_model.fit(X_full_processed, y)\n",
    "\n",
    "print(\"¡Modelo final entrenado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar predicciones finales\n",
    "print(\"\\nGenerando predicciones para test...\")\n",
    "\n",
    "# Predecir en escala log\n",
    "predictions_log = final_model.predict(X_test_processed)\n",
    "\n",
    "# Invertir transformación logarítmica\n",
    "predictions_real = np.expm1(predictions_log)\n",
    "\n",
    "# Ajustar valores negativos\n",
    "predictions_real = np.maximum(predictions_real, 0)\n",
    "\n",
    "# Aplicar factor de seguridad para evitar subestimaciones\n",
    "# Basado en el análisis: penaliza más predecir de menos\n",
    "SAFETY_FACTOR = 1.05  # 5% de margen de seguridad\n",
    "final_predictions = predictions_real * SAFETY_FACTOR\n",
    "\n",
    "print(f\"\\nFactor de seguridad aplicado: {SAFETY_FACTOR}\")\n",
    "print(f\"Esto reduce el riesgo de subestimaciones (que penalizan más).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de submisión\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids_for_submission,\n",
    "    'demand': final_predictions\n",
    "})\n",
    "\n",
    "submission_filename = 'submission_autosklearn.csv'\n",
    "submission_df.to_csv(submission_filename, index=False, sep=',')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"¡Archivo '{submission_filename}' creado con éxito!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nEstadísticas de las predicciones:\")\n",
    "print(submission_df['demand'].describe())\n",
    "\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación con el modelo de producción (EDA.ipynb)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACIÓN CON MODELO DE PRODUCCIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Cargar predicción de producción (si existe)\n",
    "    production_files = ['submission_v5_strategic.csv', 'submission2.csv', 'submission.csv']\n",
    "    production_df = None\n",
    "    \n",
    "    for prod_file in production_files:\n",
    "        if Path(prod_file).exists():\n",
    "            production_df = pd.read_csv(prod_file)\n",
    "            print(f\"\\nCargado modelo de producción: {prod_file}\")\n",
    "            break\n",
    "    \n",
    "    if production_df is not None:\n",
    "        # Comparar estadísticas\n",
    "        print(\"\\n--- Estadísticas Comparativas ---\")\n",
    "        print(f\"\\nModelo Auto-sklearn:\")\n",
    "        print(f\"  Media: {submission_df['demand'].mean():.4f}\")\n",
    "        print(f\"  Mediana: {submission_df['demand'].median():.4f}\")\n",
    "        print(f\"  Std: {submission_df['demand'].std():.4f}\")\n",
    "        print(f\"  Min: {submission_df['demand'].min():.4f}\")\n",
    "        print(f\"  Max: {submission_df['demand'].max():.4f}\")\n",
    "        \n",
    "        print(f\"\\nModelo Producción:\")\n",
    "        print(f\"  Media: {production_df['demand'].mean():.4f}\")\n",
    "        print(f\"  Mediana: {production_df['demand'].median():.4f}\")\n",
    "        print(f\"  Std: {production_df['demand'].std():.4f}\")\n",
    "        print(f\"  Min: {production_df['demand'].min():.4f}\")\n",
    "        print(f\"  Max: {production_df['demand'].max():.4f}\")\n",
    "        \n",
    "        # Diferencias\n",
    "        diff = submission_df['demand'].values - production_df['demand'].values\n",
    "        print(f\"\\n--- Diferencias (AutoML - Producción) ---\")\n",
    "        print(f\"  Media de diferencias: {np.mean(diff):.4f}\")\n",
    "        print(f\"  Std de diferencias: {np.std(diff):.4f}\")\n",
    "        print(f\"  AutoML predice más alto en {np.sum(diff > 0)} casos ({100*np.mean(diff > 0):.1f}%)\")\n",
    "        print(f\"  AutoML predice más bajo en {np.sum(diff < 0)} casos ({100*np.mean(diff < 0):.1f}%)\")\n",
    "        \n",
    "        # Visualización\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(production_df['demand'], submission_df['demand'], alpha=0.5)\n",
    "        plt.plot([0, production_df['demand'].max()], [0, production_df['demand'].max()], 'r--', lw=2)\n",
    "        plt.xlabel('Producción')\n",
    "        plt.ylabel('Auto-sklearn')\n",
    "        plt.title('Comparación de Predicciones')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(diff, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        plt.xlabel('Diferencia (AutoML - Producción)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribución de Diferencias')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('comparison_automl_vs_production.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nGráfico de comparación guardado en 'comparison_automl_vs_production.png'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo se encontró archivo de producción para comparar.\")\n",
    "        print(\"Ejecuta primero las celdas de EDA.ipynb para generar submission_v5_strategic.csv\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError al comparar con producción: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee58fa16",
   "metadata": {},
   "source": [
    "## Resumen y Conclusiones\n",
    "\n",
    "Este notebook utiliza AutoML (FLAML o auto-sklearn) para encontrar automáticamente el mejor modelo y parámetros.\n",
    "\n",
    "**Características clave:**\n",
    "1. Búsqueda automática entre múltiples algoritmos (LightGBM, RandomForest, XGBoost, CatBoost, etc.)\n",
    "2. Optimización automática de hiperparámetros\n",
    "3. Función de pérdida asimétrica que penaliza 2x las subestimaciones\n",
    "4. Factor de seguridad aplicado para reducir riesgo de predecir de menos\n",
    "5. Comparación detallada con el modelo de producción\n",
    "\n",
    "**Próximos pasos:**\n",
    "- Ajustar el `underestimation_penalty` si es necesario (actualmente 2.0)\n",
    "- Modificar el `SAFETY_FACTOR` basado en los resultados (actualmente 1.05)\n",
    "- Aumentar el `time_budget` para búsquedas más exhaustivas\n",
    "- Probar diferentes configuraciones de PCA para los embeddings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
